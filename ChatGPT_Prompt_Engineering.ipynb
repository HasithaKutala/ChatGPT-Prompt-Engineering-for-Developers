{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HasithaKutala/ChatGPT-Prompt-Engineering-for-Developers/blob/main/ChatGPT_Prompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of this course"
      ],
      "metadata": {
        "id": "Am877SbVEYSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Principles - to write clear instructions and give model time to think.\n",
        "2. Iterative prompt development \n",
        "3. Capabilites of LLM - Summarising, Inferring, Transforming, Expanding.\n",
        "4. Building a Chatbot"
      ],
      "metadata": {
        "id": "RYdcKHJrEaek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The OpenAI package provides a Python interface to interact with OpenAI's various language models and APIs."
      ],
      "metadata": {
        "id": "EJ7n4RPdcSSV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHfUeAZbXtqB",
        "outputId": "35c98338-1c5d-4753-aa95-91421d0d2154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.7 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import openai\n",
        "openai.api_key=\"use your api key here\"\n"
      ],
      "metadata": {
        "id": "bdmMhFr0X4TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines a function called get_completion that takes a prompt as input and generates a completion using the OpenAI Chat Completion API. The function sets up a conversation history, sends the prompt to the API, and retrieves the generated completion. "
      ],
      "metadata": {
        "id": "X1lXqQpgdCMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "nd3uCn1GX-At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Guidelines for prompting:**\n",
        "\n"
      ],
      "metadata": {
        "id": "ozbndhYNdbxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principle 1. Use delimiters to explain distinct parts of input. Delimiters can be anything like: ```, \"\"\", < >, <tag> </tag>, : to write clear and specific instructions.\n"
      ],
      "metadata": {
        "id": "fNjKVwRrGWYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"\"\"\n",
        "OpenAI an AI research company created and launched ChatGPT in November 2022. \\\n",
        "It was founded by a group of entrepreneurs and researchers including Elon Musk and Sam Altman in 2015. \\\n",
        "OpenAI is backed by several investors, with Microsoft being the most notable.\\\n",
        "OpenAI also created Dall-E, an AI text-to-art generator.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "Summarize the text delimited by  triple backticks \\ \n",
        "into a single sentence.\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiOzxpMjYI5l",
        "outputId": "9fd11b7a-4461-4b46-d8a9-bfe97a682e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI, an AI research company founded in 2015 by a group of entrepreneurs and researchers including Elon Musk and Sam Altman and backed by several investors including Microsoft, created and launched ChatGPT in November 2022, as well as Dall-E, an AI text-to-art generator.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above example we are creating a f-string,\n",
        "it embed expression inside {}. ''' to represent multiline string format.\n",
        "prompt- includes the main content which instructs the model to summarize the text which is enclosed within ''' into single sentence.\n",
        "backslash and f string format makes better formating.\n",
        "\n",
        "- delimiters also helps to avoid prompt injections. \n",
        "-prompt injections are if user are allowed to add input to prompt they might give conflicting instructions to model. so delimiters help model by seperating which is the instruction and the text to summarise"
      ],
      "metadata": {
        "id": "6zBTR1EpkW8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Structured output\n",
        "\n",
        "It will be helpful to ask for structured output like HTML or JSON"
      ],
      "metadata": {
        "id": "xx5vFEqG1F3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Generate a list of three latest openai api models\\\n",
        "Provide them in JSON format with the following keys: \n",
        "model_id, title , year.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NVAgrCS1XNP",
        "outputId": "5ff16b31-8a83-4073-fcbc-a1531f3d651e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"model_id\": \"davinci-codex-2021\",\n",
            "    \"title\": \"Codex\",\n",
            "    \"year\": 2021\n",
            "  },\n",
            "  {\n",
            "    \"model_id\": \"davinci-instruct-2021\",\n",
            "    \"title\": \"Instruct\",\n",
            "    \"year\": 2021\n",
            "  },\n",
            "  {\n",
            "    \"model_id\": \"davinci-landscape-2021\",\n",
            "    \"title\": \"Landscape\",\n",
            "    \"year\": 2021\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can read this into list or dictionary"
      ],
      "metadata": {
        "id": "vJJwjjRX3fbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Generate a list of three latest openai api models\\\n",
        "Provide them in HTML format with the following keys: \n",
        "model_id, title , year.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_tHd-0O3Udo",
        "outputId": "71491d52-88de-4d84-9fb0-709e79afb307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<table>\n",
            "  <tr>\n",
            "    <th>Model ID</th>\n",
            "    <th>Title</th>\n",
            "    <th>Year</th>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>GPT-3</td>\n",
            "    <td>Generative Pre-trained Transformer 3</td>\n",
            "    <td>2020</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>DALL-E</td>\n",
            "    <td>Creating Images from Text</td>\n",
            "    <td>2021</td>\n",
            "  </tr>\n",
            "  <tr>\n",
            "    <td>Codex</td>\n",
            "    <td>AI-Powered Code Generation</td>\n",
            "    <td>2021</td>\n",
            "  </tr>\n",
            "</table>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Ask the model to check conditions are satisfied or not. "
      ],
      "metadata": {
        "id": "o99w7dlW3nmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_1=f'''Open a web browser and go to the OpenAI website: \"https://openai.com\".Explore the services and products offered by OpenAI\\\n",
        "Sign up or create an account if required for the specific service or product you want to use.Follow the provided documentation or instructions to integrate OpenAI services into your applications or projects\\\n",
        "Ensure compliance with OpenAI's terms of service, usage policies, and any applicable pricing information.'''\n",
        "\n",
        "prompt= f\"\"\"\n",
        "You will be provided with text delimited by triple quotes. \n",
        "If it contains a sequence of instructions, \\ \n",
        "re-write those instructions in the following format:\n",
        "\n",
        "Step 1 - ...\n",
        "Step 2 - …\n",
        "…\n",
        "Step N - …\n",
        "\n",
        "If the text does not contain a sequence of instructions, \\ \n",
        "then simply write \\\"No steps provided.\\\"\n",
        "\n",
        "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(\"Completion for Text 1:\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmywltZW4l7p",
        "outputId": "cf5f6b84-461d-4460-d7ae-a4b60ac102af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion for Text 1:\n",
            "Step 1 - Open a web browser and go to the OpenAI website: \"https://openai.com\".\n",
            "Step 2 - Explore the services and products offered by OpenAI.\n",
            "Step 3 - Sign up or create an account if required for the specific service or product you want to use.\n",
            "Step 4 - Follow the provided documentation or instructions to integrate OpenAI services into your applications or projects.\n",
            "Step 5 - Ensure compliance with OpenAI's terms of service, usage policies, and any applicable pricing information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_2=f\"\"\"OpenAI an AI research company created and launched ChatGPT in November 2022. \\\n",
        "It was founded by a group of entrepreneurs and researchers including Elon Musk and Sam Altman in 2015. \\\n",
        "OpenAI is backed by several investors, with Microsoft being the most notable.\\\n",
        "OpenAI also created Dall-E, an AI text-to-art generator.\"\"\"\n",
        "prompt= f\"\"\"\n",
        "You will be provided with text delimited by triple quotes. \n",
        "If it contains a sequence of instructions, \\ \n",
        "re-write those instructions in the following format:\n",
        "\n",
        "Step 1 - ...\n",
        "Step 2 - …\n",
        "…\n",
        "Step N - …\n",
        "\n",
        "If the text does not contain a sequence of instructions, \\ \n",
        "then simply write \\\"No steps provided.\\\"\n",
        "\n",
        "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(\"Completion for Text 2:\")\n",
        "print(response)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdCf4ql05lCr",
        "outputId": "28e1699c-776c-4647-f6cf-b9c0412dbe6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion for Text 2:\n",
            "No steps provided.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Few-shot prompting \n",
        "It provides model with some examples to complete task and then ask it to perform the task.\n"
      ],
      "metadata": {
        "id": "1YWzZIj-5ysR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to answer in a consistent style.\n",
        "\n",
        "<child>: Teach me about patience.\n",
        "\n",
        "<grandparent>: The river that carves the deepest \\ \n",
        "valley flows from a modest spring; the \\ \n",
        "grandest symphony originates from a single note; \\ \n",
        "the most intricate tapestry begins with a solitary thread.\n",
        "\n",
        "<child>: Teach me about resilience.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_APTVGP6BhP",
        "outputId": "d533a46a-817b-482e-df1c-e3fa1472c3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<grandparent>: Resilience is like a tree that bends with the wind but never breaks. It is the ability to bounce back from adversity and keep moving forward, even when things get tough. Just like a tree that grows stronger with each storm it weathers, resilience is a quality that can be developed and strengthened over time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main purpose is using the conversation history , it will guide model to respond. \n",
        "The function makes an API call to OpenAI's Chat Completion API, passing the conversation history as input.\n",
        "The response from the API call represents the model's generated completion, which is the grandparent's response to the child's question about resilience."
      ],
      "metadata": {
        "id": "L88kB854-OZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principle 2: Give model time to think"
      ],
      "metadata": {
        "id": "iKvnY_vS_ZhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Specify the steps required to complete the task"
      ],
      "metadata": {
        "id": "_By_7qQS_lXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " text = f\"\"\"\n",
        "In a charming village, siblings Jack and Jill set out on \\ \n",
        "a quest to fetch water from a hilltop \\ \n",
        "well. As they climbed, singing joyfully, misfortune \\ \n",
        "struck—Jack tripped on a stone and tumbled \\ \n",
        "down the hill, with Jill following suit. \\ \n",
        "Though slightly battered, the pair returned home to \\ \n",
        "comforting embraces. Despite the mishap, \\ \n",
        "their adventurous spirits remained undimmed, and they \\ \n",
        "continued exploring with delight.\n",
        "\"\"\"\n",
        "# example 1\n",
        "prompt_1 = f\"\"\"\n",
        "Perform the following actions: \n",
        "1 - Summarize the following text delimited by triple \\\n",
        "backticks with 1 sentence.\n",
        "2 - Translate the summary into French.\n",
        "3 - List each name in the French summary.\n",
        "4 - Output a json object that contains the following \\\n",
        "keys: french_summary, num_names.\n",
        "\n",
        "Separate your answers with line breaks.\n",
        "\n",
        "Text:\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt_1)\n",
        "print(\"Completion for prompt 1:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W7yNVNj_xL9",
        "outputId": "7f712e3b-6acb-4083-9653-a2ec2bfe6df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion for prompt 1:\n",
            "Two siblings, Jack and Jill, go on a quest to fetch water from a well on a hilltop, but misfortune strikes and they both tumble down the hill, returning home slightly battered but with their adventurous spirits undimmed.\n",
            "\n",
            "Deux frères et sœurs, Jack et Jill, partent en quête d'eau d'un puits sur une colline, mais un malheur frappe et ils tombent tous les deux de la colline, rentrant chez eux légèrement meurtris mais avec leurs esprits aventureux intacts. \n",
            "Noms: Jack, Jill. \n",
            "\n",
            "{\n",
            "  \"french_summary\": \"Deux frères et sœurs, Jack et Jill, partent en quête d'eau d'un puits sur une colline, mais un malheur frappe et ils tombent tous les deux de la colline, rentrant chez eux légèrement meurtris mais avec leurs esprits aventureux intacts.\",\n",
            "  \"num_names\": 2\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can aslo provide the format of output"
      ],
      "metadata": {
        "id": "at6NPyZ3ApaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_2 = f\"\"\"\n",
        "Your task is to perform the following actions: \n",
        "1 - Summarize the following text delimited by \n",
        "  <> with 1 sentence.\n",
        "2 - Translate the summary into French.\n",
        "3 - List each name in the French summary.\n",
        "4 - Output a json object that contains the \n",
        "  following keys: french_summary, num_names.\n",
        "\n",
        "Use the following format:\n",
        "Text: <text to summarize>\n",
        "Summary: <summary>\n",
        "Translation: <summary translation>\n",
        "Names: <list of names in Italian summary>\n",
        "Output JSON: <json with summary and num_names>\n",
        "\n",
        "Text: <{text}>\n",
        "\"\"\"\n",
        "response = get_completion(prompt_2)\n",
        "print(\"\\nCompletion for prompt 2:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4RYVrGlAbq8",
        "outputId": "90366e9e-b775-4c37-a378-1f6820d29acc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Completion for prompt 2:\n",
            "Summary: Jack and Jill go on a quest to fetch water, but misfortune strikes and they tumble down the hill, returning home slightly battered but with their adventurous spirits undimmed. \n",
            "Translation: Jack et Jill partent en quête d'eau, mais la malchance frappe et ils dégringolent la colline, rentrant chez eux légèrement meurtris mais avec leurs esprits aventureux intacts.\n",
            "Names: Jack, Jill\n",
            "Output JSON: {\"french_summary\": \"Jack et Jill partent en quête d'eau, mais la malchance frappe et ils dégringolent la colline, rentrant chez eux légèrement meurtris mais avec leurs esprits aventureux intacts.\", \"num_names\": 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Instruct the model to check beofre rushing to conclusion"
      ],
      "metadata": {
        "id": "5NZinNO9AxVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Determine if the student's solution is correct or not.\n",
        "\n",
        "Question:\n",
        "I'm building a solar power installation and I need \\\n",
        " help working out the financials. \n",
        "- Land costs $100 / square foot\n",
        "- I can buy solar panels for $250 / square foot\n",
        "- I negotiated a contract for maintenance that will cost \\ \n",
        "me a flat $100k per year, and an additional $10 / square \\\n",
        "foot\n",
        "What is the total cost for the first year of operations \n",
        "as a function of the number of square feet.\n",
        "\n",
        "Student's Solution:\n",
        "Let x be the size of the installation in square feet.\n",
        "Costs:\n",
        "1. Land cost: 100x\n",
        "2. Solar panel cost: 250x\n",
        "3. Maintenance cost: 100,000 + 100x\n",
        "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azRfQX1UA3pM",
        "outputId": "342ae1a6-a2c2-475a-e9cf-66862b01cc4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The student's solution is correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here the output is wrong , the student answer is not accurate. so we need to  instruct the model."
      ],
      "metadata": {
        "id": "JyI7hUDkBnFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to determine if the student's solution \\\n",
        "is correct or not.\n",
        "To solve the problem do the following:\n",
        "- First, work out your own solution to the problem. \n",
        "- Then compare your solution to the student's solution \\ \n",
        "and evaluate if the student's solution is correct or not. \n",
        "Don't decide if the student's solution is correct until \n",
        "you have done the problem yourself.\n",
        "\n",
        "Use the following format:\n",
        "Question:\n",
        "```\n",
        "question here\n",
        "```\n",
        "Student's solution:\n",
        "```\n",
        "student's solution here\n",
        "```\n",
        "Actual solution:\n",
        "```\n",
        "steps to work out the solution and your solution here\n",
        "```\n",
        "Is the student's solution the same as actual solution \\\n",
        "just calculated:\n",
        "```\n",
        "yes or no\n",
        "```\n",
        "Student grade:\n",
        "correct or incorrect\n",
        "```\n",
        "\n",
        "Question:\n",
        "```\n",
        "I'm building a solar power installation and I need help \\\n",
        "working out the financials. \n",
        "- Land costs $100 / square foot\n",
        "- I can buy solar panels for $250 / square foot\n",
        "- I negotiated a contract for maintenance that will cost \\\n",
        "me a flat $100k per year, and an additional $10 / square \\\n",
        "foot\n",
        "What is the total cost for the first year of operations \\\n",
        "as a function of the number of square feet.\n",
        "``` \n",
        "Student's solution:\n",
        "```\n",
        "Let x be the size of the installation in square feet.\n",
        "Costs:\n",
        "1. Land cost: 100x\n",
        "2. Solar panel cost: 250x\n",
        "3. Maintenance cost: 100,000 + 100x\n",
        "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
        "```\n",
        "Actual solution:\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8S_nZQeB7u9",
        "outputId": "76fcaa18-7d3d-4934-f3eb-b68f44c98d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let x be the size of the installation in square feet.\n",
            "Costs:\n",
            "1. Land cost: 100x\n",
            "2. Solar panel cost: 250x\n",
            "3. Maintenance cost: 100,000 + 10x\n",
            "Total cost: 100x + 250x + 100,000 + 10x = 360x + 100,000\n",
            "\n",
            "Is the student's solution the same as actual solution just calculated:\n",
            "No\n",
            "\n",
            "Student grade:\n",
            "Incorrect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here model goes through the instructions given and performs its own calculations then compares the student grade with the actual and gives that it is incorrect. \n",
        "The main purpose is to show how asking the model to do a calculation itself and breaking  down the task into steps to give the model more time to think to get more accurate responses."
      ],
      "metadata": {
        "id": "I-QwRr1DDfF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Limitations**\n",
        "\n",
        "Model doesnot know its boundary of knowledge very well, it means that model will hallucinate and makeup things that sound correct but actually not."
      ],
      "metadata": {
        "id": "szrGlMjoEIq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Tell me about Toothbrush by openai\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnD2uBFuEPgc",
        "outputId": "bacb1503-2d38-41cc-c8a5-8df095acbfb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toothbrush is a language model developed by OpenAI, which is designed to generate human-like text. It is a part of the GPT (Generative Pre-trained Transformer) family of models, which are trained on large amounts of text data to generate coherent and contextually relevant text.\n",
            "\n",
            "Toothbrush is specifically trained on dental and oral health-related text, making it an ideal tool for generating content related to dental hygiene, oral health, and related topics. It can be used to generate articles, blog posts, social media posts, and other types of content related to dental health.\n",
            "\n",
            "The model is trained on a massive dataset of dental and oral health-related text, which includes scientific research papers, dental textbooks, and other relevant sources. This allows it to generate text that is both accurate and informative, while also being engaging and easy to read.\n",
            "\n",
            "Overall, Toothbrush is a powerful tool for generating high-quality content related to dental health, and it has the potential to be a valuable resource for dentists, dental hygienists, and other professionals in the dental industry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no toothbrush provided by openai so it just give some imagined output. So in order to reduce hallucinations first find the relevant information and then answer the question based on the relevant information."
      ],
      "metadata": {
        "id": "9dizVy1MFmNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Iterative Prompt Development**"
      ],
      "metadata": {
        "id": "-26U9WN4GcCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "prompt guidelines - Be clear and specific and analyse why result not giving desired output. Then refine the idea and the prompt."
      ],
      "metadata": {
        "id": "fJvAUMp4GuBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OpenAI = \"\"\"\n",
        "OVERVIEW\n",
        "OpenAI an AI research company created and launched ChatGPT in November 2022. \\\n",
        "It was founded by a group of entrepreneurs and researchers including Elon Musk and Sam Altman in 2015. \\\n",
        "OpenAI is backed by several investors, with Microsoft.\n",
        "\n",
        "\n",
        "Models\n",
        "- GPT-4\n",
        "- GPT-3.5\n",
        "- GPT-3\n",
        "\n",
        "OPTIONS\n",
        "-  gpt 4 can understand as well as generate natural language or code\n",
        "-  gpt3.5 can infering sentiment analysis\n",
        "- gpt 3 can summarizing, transforming and expanding\n",
        "\n",
        "Max Tokens\n",
        "\n",
        "-gpt-4 has 8,192 tokens\n",
        "-gpt-3.5 has 4,096 tokens\n",
        "-gpt-3 has 2,049 tokens\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jf4OuFS5H-Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to help a developing team create a \n",
        "description for a openai website of a model based \n",
        "on a technical fact sheet.\n",
        "\n",
        "Write a model description based on the information \n",
        "provided in the technical specifications delimited by \n",
        "triple backticks.\n",
        "\n",
        "Use at most 3 sentences with 50 words.\n",
        "\n",
        "Technical specifications: ```{OpenAI}```\n",
        "\"\"\"\n",
        "response_1 = get_completion(prompt)\n",
        "print(response_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9w07FQTIMVk",
        "outputId": "a9e1d501-8ae8-445e-c470-1295d571456c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT is an AI model developed by OpenAI that can understand and generate natural language or code, infer sentiment analysis, and perform summarization, transformation, and expansion. The model comes in three versions: GPT-4 with 8,192 tokens, GPT-3.5 with 4,096 tokens, and GPT-3 with 2,049 tokens. OpenAI was founded in 2015 by a group of entrepreneurs and researchers, including Elon Musk and Sam Altman, and is backed by several investors, including Microsoft.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. we can tell the model to focus on specific details that are relevant to users"
      ],
      "metadata": {
        "id": "mr3pz4K3J6PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "\n",
        "Your task is to help a developing team create a \n",
        "description for a openai website of a model based \n",
        "on a technical fact sheet.\n",
        "\n",
        "Write a model description based on the information \n",
        "provided in the technical specifications delimited by \n",
        "triple backticks.\n",
        "\n",
        "The description is intended for best model with most tokens, \n",
        "so should be understanding in nature.\n",
        "\n",
        "Use at most 50 words.\n",
        "\n",
        "Technical specifications: ```{OpenAI}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oetSdvLMKAbY",
        "outputId": "7dcb51a2-13d5-4638-c8cf-099698214feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT, created by OpenAI, is a powerful language model that can understand and generate natural language or code. It can also perform sentiment analysis, summarization, transformation, and expansion. With GPT-4 having 8,192 tokens, GPT-3.5 having 4,096 tokens, and GPT-3 having 2,049 tokens, ChatGPT is the best model for handling large amounts of text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to include specific words then "
      ],
      "metadata": {
        "id": "dfzI3_1pKXeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to help a developing team create a \n",
        "description for a openai website of a model based \n",
        "on a technical fact sheet.\n",
        "\n",
        "Write a model description based on the information \n",
        "provided in the technical specifications delimited by \n",
        "triple backticks.\n",
        "\n",
        "\n",
        "At the end of the description, include model options with its type\n",
        "and technical specification.\n",
        "\n",
        "Use at most 50 words.\n",
        "\n",
        "Technical specifications: ```{OpenAI}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3A-IClGKWHO",
        "outputId": "8e945de8-54bd-48e5-85ca-89e7608b7948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT is an AI model developed by OpenAI for natural language processing. It can understand and generate natural language or code, infer sentiment analysis, and summarize, transform, and expand text. The model comes in three versions: GPT-4 with 8,192 tokens, GPT-3.5 with 4,096 tokens, and GPT-3 with 2,049 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Description needs a table of dimensions\n",
        "Ask it to extract information and organize it in a table."
      ],
      "metadata": {
        "id": "Zq3X5iR-K-AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "\n",
        "Your task is to help a developing team create a \n",
        "description for a openai website of a model based \n",
        "on a technical fact sheet.\n",
        "\n",
        "Write a model description based on the information \n",
        "provided in the technical specifications delimited by \n",
        "triple backticks.\n",
        "\n",
        "\n",
        "At the end of the description, include model options with its type\n",
        "and technical specification.\n",
        "\n",
        "\n",
        "After the description, include a table that gives the \n",
        "model's dimensions. The table should have two columns.\n",
        "In the first column include the name of the model. \n",
        "In the second column include the tokens.\n",
        "\n",
        "Give the table the title 'OpenAI API Models'.\n",
        "\n",
        "Format everything as HTML that can be used in a website. \n",
        "Place the description in a <div> element.\n",
        "\n",
        "Technical specifications: ```{OpenAI}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0jx48c_KwME",
        "outputId": "adee2ee3-ef13-4bc3-c3e7-4c6939771c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<div>\n",
            "  <h2>ChatGPT: OpenAI's Language Model</h2>\n",
            "  <p>ChatGPT is a state-of-the-art language model developed by OpenAI, an AI research company founded by a group of entrepreneurs and researchers including Elon Musk and Sam Altman in 2015. ChatGPT was launched in November 2022 and is backed by several investors, including Microsoft.</p>\n",
            "  <p>ChatGPT comes in three different models: GPT-4, GPT-3.5, and GPT-3. Each model is capable of understanding as well as generating natural language or code, inferring sentiment analysis, summarizing, transforming, and expanding text.</p>\n",
            "  \n",
            "  <h3>Model Options</h3>\n",
            "  <ul>\n",
            "    <li>Understand as well as generate natural language or code</li>\n",
            "    <li>Infering sentiment analysis</li>\n",
            "    <li>Summarizing, transforming, and expanding</li>\n",
            "  </ul>\n",
            "  \n",
            "  <h3>OpenAI API Models</h3>\n",
            "  <table>\n",
            "    <thead>\n",
            "      <tr>\n",
            "        <th>Model</th>\n",
            "        <th>Max Tokens</th>\n",
            "      </tr>\n",
            "    </thead>\n",
            "    <tbody>\n",
            "      <tr>\n",
            "        <td>GPT-4</td>\n",
            "        <td>8,192</td>\n",
            "      </tr>\n",
            "      <tr>\n",
            "        <td>GPT-3.5</td>\n",
            "        <td>4,096</td>\n",
            "      </tr>\n",
            "      <tr>\n",
            "        <td>GPT-3</td>\n",
            "        <td>2,049</td>\n",
            "      </tr>\n",
            "    </tbody>\n",
            "  </table>\n",
            "</div>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check whether this works or not load pyhton libraries to view html"
      ],
      "metadata": {
        "id": "QWAj3KKsLnsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML"
      ],
      "metadata": {
        "id": "2VER3z0xLktU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "7I_tL2SWLwGc",
        "outputId": "b4481a35-de18-4e41-b2f5-38b9db9f179f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div>\n",
              "  <h2>ChatGPT: OpenAI's Language Model</h2>\n",
              "  <p>ChatGPT is a state-of-the-art language model developed by OpenAI, an AI research company founded by a group of entrepreneurs and researchers including Elon Musk and Sam Altman in 2015. ChatGPT was launched in November 2022 and is backed by several investors, including Microsoft.</p>\n",
              "  <p>ChatGPT comes in three different models: GPT-4, GPT-3.5, and GPT-3. Each model is capable of understanding as well as generating natural language or code, inferring sentiment analysis, summarizing, transforming, and expanding text.</p>\n",
              "  \n",
              "  <h3>Model Options</h3>\n",
              "  <ul>\n",
              "    <li>Understand as well as generate natural language or code</li>\n",
              "    <li>Infering sentiment analysis</li>\n",
              "    <li>Summarizing, transforming, and expanding</li>\n",
              "  </ul>\n",
              "  \n",
              "  <h3>OpenAI API Models</h3>\n",
              "  <table>\n",
              "    <thead>\n",
              "      <tr>\n",
              "        <th>Model</th>\n",
              "        <th>Max Tokens</th>\n",
              "      </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "      <tr>\n",
              "        <td>GPT-4</td>\n",
              "        <td>8,192</td>\n",
              "      </tr>\n",
              "      <tr>\n",
              "        <td>GPT-3.5</td>\n",
              "        <td>4,096</td>\n",
              "      </tr>\n",
              "      <tr>\n",
              "        <td>GPT-3</td>\n",
              "        <td>2,049</td>\n",
              "      </tr>\n",
              "    </tbody>\n",
              "  </table>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarizing "
      ],
      "metadata": {
        "id": "UGT8zdPEpmSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_history=''''OpenAI, founded in 2015, is an artificial intelligence research laboratory. \\\n",
        "It has pioneered breakthroughs in natural language processing and deep learning.\\\n",
        " OpenAI's flagship model, GPT, has transformed various industries. \\\n",
        " In 2021, OpenAI launched ChatGPT, an advanced language model designed to engage\\\n",
        "  in conversational interactions and provide information across multiple domains.'''"
      ],
      "metadata": {
        "id": "v6yPqJDzrIiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "summarization can be performed by using word/sentence/character limit \n"
      ],
      "metadata": {
        "id": "PjoYF1lDslTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\" \n",
        "task is to generate a short summary about history of openai\n",
        "\n",
        "summarize it below, delimited by triple backticks, in at most 30 words.\n",
        "\n",
        "history:'''{openai_history}'''\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5OalbeMstEI",
        "outputId": "953ad65b-081d-4ac4-be0a-2a09632fb2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```OpenAI, founded in 2015, is an AI research lab known for breakthroughs in natural language processing and deep learning. Its flagship model, GPT, has transformed industries. In 2021, OpenAI launched ChatGPT for conversational interactions and information across domains.```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can summarize by focusing on particular fields "
      ],
      "metadata": {
        "id": "3viNlGe9vp0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "task is to summazie a short summary of chatgpt history \\\n",
        "to give feedback to developers.\n",
        "\n",
        "summarize the below, delimited by triple backticks, in at most 30 words, focus on aspects\\\n",
        "that mention the dates of founded\n",
        "history:'''{openai_history}\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvTmfKBtvwOV",
        "outputId": "c7979ebd-dc5e-4d42-9827-81dff05d9089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI, founded in 2015, developed GPT, a language model that transformed industries. In 2021, they launched ChatGPT, an advanced conversational language model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "task is to summazie a short summary of chatgpt history \\\n",
        "to give feeback to developers, responsible of characterstics.\n",
        "\n",
        "summarize the below, delimited by triple backticks, in at most 30 words, focus on aspects\\\n",
        "that are relevant to the charaterstics of the model.\n",
        "history:'''{openai_history}\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wCHiLdGyCrp",
        "outputId": "29fd3414-b041-4979-ba69-48b1cb755294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI, founded in 2015, has pioneered breakthroughs in natural language processing and deep learning. Its flagship model, GPT, has transformed various industries. In 2021, OpenAI launched ChatGPT, an advanced language model designed for conversational interactions and providing information across multiple domains. Relevant to the characteristics of the model are its advanced language processing capabilities and ability to engage in conversational interactions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summaries include topics that are not related to the topic of focus."
      ],
      "metadata": {
        "id": "9N7EyJUIyfXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the information instead of summarizing "
      ],
      "metadata": {
        "id": "CTqiVkRfytI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "task is to extract relevant information from history of openai to give \n",
        "feedback to developers.\n",
        "\n",
        "From the history, delimited by triple quotes\\\n",
        "extract the information relevant to types of models released and limit to 30 words.\n",
        "\n",
        "History:'''{openai_history}\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itYQ7vjMysBL",
        "outputId": "b0ed6a34-0573-4f81-cde7-698ef93b1ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI has released GPT and ChatGPT, two natural language processing models that have transformed industries and engage in conversational interactions across multiple domains.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarizing multiple text at a time "
      ],
      "metadata": {
        "id": "DjxDt5LR2A0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "review_1= openai_history\n",
        "\n",
        "review_2=\"\"\"\n",
        "GPT-3 (Generative Pre-trained Transformer 3): GPT-3 is a groundbreaking language model known\\\n",
        " for its impressive natural language processing capabilities. It can generate coherent and \\\n",
        " contextually relevant text but may sometimes produce inaccurate or nonsensical responses\"\"\"\n",
        "\n",
        "review_3=\"\"\"\n",
        "DALL-E: DALL-E is a model trained to generate images from textual descriptions. \\\n",
        "It can create unique and imaginative visuals, but its output can be unpredictable\\\n",
        " and sometimes lacks coherence with the provided description.\"\"\"\n",
        "\n",
        "review_4=\"\"\"\n",
        "CLIP (Contrastive Language-Image Pretraining): CLIP is a model that can understand both images\\\n",
        " and text, enabling tasks such as image classification based on textual prompts. It demonstrates\\\n",
        "  impressive cross-modal understanding, but its performance may vary across different domains.\n",
        "\"\"\"\n",
        "\n",
        "review_5=\"\"\"\n",
        "Codex: Codex is a language model specifically designed for programming tasks. It can generate code\\\n",
        " based on natural language prompts and has the potential to assist developers in various \\\n",
        " programming-related activities. However, it may produce code that is \\\n",
        " syntactically correct but conceptually flawed.\"\"\"\n",
        "\n",
        "reviews=[review_1,review_2, review_3,review_4,review_5] "
      ],
      "metadata": {
        "id": "H_4bMv7H2DbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(reviews)):\n",
        "\n",
        "  prompt=f\"\"\"\n",
        "   task is to generate summary of model review \n",
        "   summarize the reveiw, delimited by triple backticks in 20 words.\n",
        "\n",
        "   review:'''{reviews[i]}'''\n",
        "   \"\"\"\n",
        "  response=get_completion(prompt)\n",
        "  print(i,response,\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "cSKQRwOG5Ng3",
        "outputId": "81e3bb21-69c7-4357-ce96-d26eadb7471a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 OpenAI is an AI research lab that has made significant breakthroughs in natural language processing and deep learning. Their flagship model, GPT, has transformed various industries. They recently launched ChatGPT, an advanced language model designed for conversational interactions and providing information across multiple domains. \n",
            "\n",
            "1 GPT-3 is a powerful language model with impressive natural language processing capabilities, but may produce inaccurate or nonsensical responses. \n",
            "\n",
            "2 DALL-E is a model that generates images from text, producing imaginative visuals but sometimes lacking coherence with the description. \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-ac9782835dba>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m    \u001b[0mreview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'''{reviews[i]}'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m    \"\"\"\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-8db33b03e43d>\u001b[0m in \u001b[0;36mget_completion\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         )\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             return (\n\u001b[0;32m--> 624\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    625\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    688\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-tzZltcQ0P7sUaBKrytuzVyUz on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferring \n",
        "\n",
        "Infer the sentiment from product reviews and articles"
      ],
      "metadata": {
        "id": "SjAWUo2E6JU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_review=\"\"\"A waste of time with There is no trial feature. I came across\\\n",
        " this online and accessed their site for the first time.\\\n",
        "There was an option that said \"try dall-e,\" and I believed it was free to test,\\\n",
        "as are most apps and websites. When I registered, I verified my mobile, but it\\\n",
        "told me I didn't have any credits or credits. It was a complete waste of time and energy.\"\"\""
      ],
      "metadata": {
        "id": "oKMdEgqD6V2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment analysis**"
      ],
      "metadata": {
        "id": "6bJ3uJGB7kef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "what is the sentiment of the following product review which is delimited \\\n",
        "with triple backticks?\n",
        " \n",
        "Review:'''{openai_review}'''\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg5ENSDt6_1w",
        "outputId": "e5a38976-348a-4648-9b6b-23f1401d1221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the product review is negative.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "what is the sentiment of the following product review which is delimited \\\n",
        "with triple backticks?\n",
        " \n",
        "give answer in a single word, either postive\\negative \n",
        "Review:'''{openai_review}'''\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvoCQlI37vj5",
        "outputId": "f5f1d6ab-8b4c-4b8d-f4d7-3d5e9c01bb0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Types of emotions**"
      ],
      "metadata": {
        "id": "PSatlY_hYp7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "Identify a list of emotions that the writer of the review is expressing.\n",
        "include no more than five items in list. Format your answer as list of lower-case seperated by comas.\n",
        "\n",
        "Review:'''{openai_review}'''\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT6R6WDFYY5H",
        "outputId": "c2f48864-6a07-4128-efbb-344365ea5d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "disappointment, frustration, annoyance, confusion, regret\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai_review1='''ChatGPT, it's a great tool\\\n",
        " for any kind of business, even for developers. \\\n",
        " The truth, it can be used as a companion for your daily basis work,\\\n",
        "  but is not meant to be your lifeguard. You will need your own skills.\\\n",
        "   Don't expect to get a copywriter job 100% based on ChatGPT'''\n",
        "prompt=f\"\"\"\n",
        "Identify a list of emotions that the writer of the review is expressing.\n",
        "include no more than five items in list. Format your answer as list of lower-case seperated by comas.\n",
        "aslo write the sentiment of the review\n",
        "\n",
        "Review:'''{openai_review1}'''\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_for04yZ2ZX",
        "outputId": "aa884989-59e2-4944-fd73-ef66bb8549ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive, appreciative, realistic, cautionary, informative\n",
            "\n",
            "Sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "Is the review expresing anger?\n",
        "give your answer as yes or no\n",
        "\n",
        "Review:'''{openai_review1}'''\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c42GmlOKaPTm",
        "outputId": "643838a3-0117-4c1c-8c0c-5fa03cbc5dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting information from the text "
      ],
      "metadata": {
        "id": "J4aFQDbpazRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "identify the following items from the review text.\n",
        "- model discussed in the review by the user \n",
        "- 3 uses of it \n",
        "\n",
        "The review is delimited with triple backticks.\n",
        "Format your response as JSON with \"Model\" and \"uses\" as the keys.\n",
        "If their is no information for above keys then use value as \"unknown\"\n",
        "\n",
        "Make response short.\n",
        "Review:'''{openai_review1}\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEf51Dd_aypH",
        "outputId": "5b5256b8-87fe-41cd-e4f7-383cb7148533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Model\": \"ChatGPT\",\n",
            "  \"uses\": [\n",
            "    \"Companion for daily basis work\",\n",
            "    \"Great tool for any kind of business\",\n",
            "    \"Useful for developers\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mutliple tasks at once"
      ],
      "metadata": {
        "id": "4_fDs4FlcA0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "identify the following items from the review text.\n",
        "- model discussed in the review by the user \n",
        "- best use of it \n",
        "- sentiment of the review\n",
        "- Is the review expressing anger? (true or false)\n",
        "\n",
        "The review is delimited with triple backticks.\n",
        "Format your response as JSON with \"Model\" , \"uses\",\"sentiment\",\"anger\" as the keys.\n",
        "If their is no information for above keys then use value as \"unknown\"\n",
        "\n",
        "Make response short.\n",
        "Review:'''{openai_review1}\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzwCN41LcCZx",
        "outputId": "e0fc1f20-f4c5-47e6-d921-2a70e047cfc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Model\": \"ChatGPT\",\n",
            "  \"uses\": \"Companion for daily basis work\",\n",
            "  \"sentiment\": \"Positive\",\n",
            "  \"anger\": false\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inferring topics** "
      ],
      "metadata": {
        "id": "m_G0MGIUcWNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "story=\"\"\" OpenAI has delivered a series of impressive advances in AI that\\\n",
        "works with language in recent years by taking existing machine-learning algorithms\\\n",
        "and scaling them up to previously unimagined size. GPT-4, the latest of those projects\\\n",
        "was likely trained using trillions of words of text and many thousands of\\\n",
        "powerful computer chips. The process cost over $100 million\\\n",
        "But the company’s CEO, Sam Altman, says further progress will not come from making models bigger.\\\n",
        "I think we're at the end of the era where it's going to be these, like, giant, giant models,”\\\n",
        "he told an audience at an event held at MIT late last week. “We'll make them better in other ways.\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "jeSOwf0vcUpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "Determine five topics discussed in text, which is delimited by triple backticks.\n",
        "Make each item one or two words long.\n",
        "Format your response as a list items seperated by commas.\n",
        "text: '''{story}'''\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWQFWLqxc9VS",
        "outputId": "8fbbb0c8-2080-4054-ce4d-50257870a8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI, AI advances, GPT-4, machine-learning algorithms, Sam Altman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.split(sep=',')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvKLcprSdnIl",
        "outputId": "a86ce945-17f8-4734-bb1e-c811caa48299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['OpenAI',\n",
              " ' AI advances',\n",
              " ' GPT-4',\n",
              " ' machine-learning algorithms',\n",
              " ' Sam Altman']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking which of the topics are covered in the text or not "
      ],
      "metadata": {
        "id": "KzBBdvQjfEq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_list=[\"AI language\",\"google\",\"beta\",\"alexa\",\"siri\",\"OpenAI\"]"
      ],
      "metadata": {
        "id": "qOfu48hVdux1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "Determine whether each item in the following list of \\\n",
        "topics is a topic in the text below, which\n",
        "is delimited with triple backticks.\n",
        "\n",
        "Give your answer as list with 0 or 1 for each topic.\\\n",
        "\n",
        "List of topics: {\", \".join(topic_list)}\n",
        "\n",
        "Text sample: '''{story}'''\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLT_Au-mfIMf",
        "outputId": "9d3ff744-ee48-44fd-e07e-d058fcb0958d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI language: 1\n",
            "google: 0\n",
            "beta: 0\n",
            "alexa: 0\n",
            "siri: 0\n",
            "OpenAI: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If its present then the value of the key is 1 , if not value is 0. \n",
        "{\", \".join(topic_list)}\" is used to dynamically insert the list of topics within the prompt. By using the \".join()\" method with a comma separator, the individual elements of the \"topic_list\" are concatenated into a single string."
      ],
      "metadata": {
        "id": "S530Fi4igGeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " above code  is responsible for generating the response from the language model based on the given prompt, while the below code  processes the response and checks if a specific topic ('nasa') has a value of 1 in the response."
      ],
      "metadata": {
        "id": "ii8YZeWqoqXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_dict = {i.split(': ')[0]: int(i.split(': ')[1]) for i in response.split(sep='\\n')}\n",
        "if topic_dict['OpenAI'] == 1:\n",
        "    print(\"ALERT: New OpenAi story!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4l9dAp4nOhi",
        "outputId": "9a788737-b7f1-4676-9e4d-3bd73e087770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALERT: New OpenAi story!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transforming**\n",
        "\n",
        "Using llm for text transformation like language translation, spelling check tone adjustment and format conversion"
      ],
      "metadata": {
        "id": "LezhOgqdo4lg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Translation"
      ],
      "metadata": {
        "id": "d9gLvMSypGgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "Translate the following English text to Spanish: \\ \n",
        "```Hi, I would like to order a blender```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXJ4ODhqpEvB",
        "outputId": "79077f5b-55c2-4df8-aeba-aedd97d4cf6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola, me gustaría ordenar una licuadora.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Tell me which language this is: \n",
        "```Combien coûte le lampadaire?```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGUy50OfpYIi",
        "outputId": "01ced5b6-d44b-429e-988b-f94458dc5a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is French.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "multiple transaltions at once"
      ],
      "metadata": {
        "id": "subKbJ8mpfiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Translate the following  text to French and Spanish\n",
        "and English pirate: \\\n",
        "```I want to order a basketball```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-PcZ5rPpbfC",
        "outputId": "a509fc5c-dfd1-4532-acc1-476d8c4143f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French pirate: ```Je veux commander un ballon de basket```\n",
            "Spanish pirate: ```Quiero pedir una pelota de baloncesto```\n",
            "English pirate: ```I want to order a basketball```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Translate the following text to Spanish in both the \\\n",
        "formal and informal forms: \n",
        "'Would you like to order a pillow?'\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unzZTzn0pm5X",
        "outputId": "71ce428f-7cac-4263-b9d5-57691b1de12d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formal: ¿Le gustaría ordenar una almohada?\n",
            "Informal: ¿Te gustaría ordenar una almohada?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Universal translator"
      ],
      "metadata": {
        "id": "MvDH_VA7qrGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_messages = [\n",
        "  \"La performance du système est plus lente que d'habitude.\",  # System performance is slower than normal         \n",
        "  \"Mi monitor tiene píxeles que no se iluminan.\",              # My monitor has pixels that are not lighting\n",
        "  \"Il mio mouse non funziona\",                                 # My mouse is not working\n",
        "  \"Mój klawisz Ctrl jest zepsuty\",                             # My keyboard has a broken control key\n",
        "  \"我的屏幕在闪烁\"                                               # My screen is flashing\n",
        "] "
      ],
      "metadata": {
        "id": "1RPxFeRRqt-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for issue in user_messages:\n",
        "    prompt = f\"Tell me what language this is: ```{issue}```\"\n",
        "    lang = get_completion(prompt)\n",
        "    print(f\"Original message ({lang}): {issue}\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Translate the following  text to English \\\n",
        "    and Korean: ```{issue}```\n",
        "    \"\"\"\n",
        "    response = get_completion(prompt)\n",
        "    print(response, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "rLCXMG5TqwmQ",
        "outputId": "14f2cf60-ec2d-401b-bbf5-20c18fdadb8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original message (This is French.): La performance du système est plus lente que d'habitude.\n",
            "English: The system performance is slower than usual.\n",
            "Korean: 시스템 성능이 평소보다 느립니다. \n",
            "\n",
            "Original message (This is Spanish.): Mi monitor tiene píxeles que no se iluminan.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-c154fe0a871f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0mKorean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0missue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-8db33b03e43d>\u001b[0m in \u001b[0;36mget_completion\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         )\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             return (\n\u001b[0;32m--> 624\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    625\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    688\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-tzZltcQ0P7sUaBKrytuzVyUz on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Tone transformation"
      ],
      "metadata": {
        "id": "cc3GXAh6q7QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Translate the following from slang to a business letter: \n",
        "'Dude, This is Joe, im not coming tommorow.'\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M6BSI3tq5qb",
        "outputId": "034c3433-4c6c-4627-be62-beca750bc889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear Sir/Madam,\n",
            "\n",
            "I am writing to inform you that I, Joe, will not be able to attend work tomorrow.\n",
            "\n",
            "Thank you for your understanding.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "Joe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Format conversion \n",
        "\n",
        "It can convert the format like html , JSON,xml, markdown. The prompt should describe the input and output formats.\n"
      ],
      "metadata": {
        "id": "CXOiC27krhT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_json = { \"resturant employees\" :[ \n",
        "    {\"name\":\"Shyam\", \"email\":\"shyamjaiswal@gmail.com\"},\n",
        "    {\"name\":\"Bob\", \"email\":\"bob32@gmail.com\"},\n",
        "    {\"name\":\"Jai\", \"email\":\"jai87@gmail.com\"}\n",
        "]}\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Translate the following python dictionary from JSON to an HTML \\\n",
        "table with column headers and title: {data_json}\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmvQE3yerxrt",
        "outputId": "0b2eea26-e386-4510-f263-cb58b31c55ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<table>\n",
            "  <caption>Restaurant Employees</caption>\n",
            "  <thead>\n",
            "    <tr>\n",
            "      <th>Name</th>\n",
            "      <th>Email</th>\n",
            "    </tr>\n",
            "  </thead>\n",
            "  <tbody>\n",
            "    <tr>\n",
            "      <td>Shyam</td>\n",
            "      <td>shyamjaiswal@gmail.com</td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "      <td>Bob</td>\n",
            "      <td>bob32@gmail.com</td>\n",
            "    </tr>\n",
            "    <tr>\n",
            "      <td>Jai</td>\n",
            "      <td>jai87@gmail.com</td>\n",
            "    </tr>\n",
            "  </tbody>\n",
            "</table>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown, Latex, HTML, JSON\n",
        "display(HTML(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "g4t4wfAdsADn",
        "outputId": "679537e1-4bf5-4ce7-b08f-33f55354a499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table>\n",
              "  <caption>Restaurant Employees</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>Name</th>\n",
              "      <th>Email</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>Shyam</td>\n",
              "      <td>shyamjaiswal@gmail.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Bob</td>\n",
              "      <td>bob32@gmail.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Jai</td>\n",
              "      <td>jai87@gmail.com</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_json = { \"resturant employees\" :[ \n",
        "    {\"name\":\"Shyam\", \"email\":\"shyamjaiswal@gmail.com\"},\n",
        "    {\"name\":\"Bob\", \"email\":\"bob32@gmail.com\"},\n",
        "    {\"name\":\"Jai\", \"email\":\"jai87@gmail.com\"}\n",
        "]}\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Translate the following python dictionary from JSON markdown\\\n",
        " with column headers and title: {data_json}\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLCK4Fp1r5ch",
        "outputId": "41296265-3dbc-48b4-b34f-096ff997b8d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| resturant employees |             |\n",
            "|---------------------|-------------|\n",
            "| name                | email       |\n",
            "| Shyam               | shyamjaiswal@gmail.com |\n",
            "| Bob                 | bob32@gmail.com |\n",
            "| Jai                 | jai87@gmail.com |\n",
            "\n",
            "{'resturant employees': [{'name': 'Shyam', 'email': 'shyamjaiswal@gmail.com'}, {'name': 'Bob', 'email': 'bob32@gmail.com'}, {'name': 'Jai', 'email': 'jai87@gmail.com'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Grammar check \n"
      ],
      "metadata": {
        "id": "W3Fc-KuyuIOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To signal to the LLM that you want it to proofread your text, you instruct the model to 'proofread' or 'proofread and correct'."
      ],
      "metadata": {
        "id": "Qzoh8erZuW2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"\"\"\n",
        "  The girl with the black and white puppies have a ball.\",  # The girl has a ball.\n",
        "  \"\"\"\n",
        "\n",
        "prompt = f\"\"\"Proofread and correct the following text\n",
        "    and rewrite the corrected version. If you don't find\n",
        "    and errors, just say \"No errors found\". Don't use \n",
        "    any punctuation around the text:\n",
        "    ```{text}```\n",
        "    \"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHmPmlhxuXkR",
        "outputId": "88b4738e-b0e8-4202-f4d6-91ea0781de8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The girl with the black and white puppies has a ball.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"\"\"\n",
        "Got this for my daughter for her birthday cuz she keeps taking \\\n",
        "mine from my room.  Yes, adults also like pandas too.  She takes \\\n",
        "it everywhere with her, and it's super soft and cute.  One of the \\\n",
        "ears is a bit lower than the other, and I don't think that was \\\n",
        "designed to be asymmetrical. It's a bit small for what I paid for it \\\n",
        "though. I think there might be other options that are bigger for \\\n",
        "the same price.  It arrived a day earlier than expected, so I got \\\n",
        "to play with it myself before I gave it to my daughter.\n",
        "\"\"\"\n",
        "prompt = f\"proofread and correct this review: ```{text}```\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx2Mhnn_vVFh",
        "outputId": "4074a282-b664-4073-fede-ced1aa3c2bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I got this for my daughter's birthday because she keeps taking mine from my room. Yes, adults also like pandas too. She takes it everywhere with her, and it's super soft and cute. However, one of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical. Additionally, it's a bit small for what I paid for it. I think there might be other options that are bigger for the same price. On the positive side, it arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install redlines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWPMPYVqv_HY",
        "outputId": "71c175dc-fb0c-4e5a-f5bc-b03642c0f24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: redlines in /usr/local/lib/python3.10/dist-packages (0.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see the changes in red colour "
      ],
      "metadata": {
        "id": "i9kA_iGrxWMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown, HTML\n",
        "\n",
        "# Get the redlines output in Markdown format\n",
        "output_markdown = diff.output_markdown\n",
        "\n",
        "# Add HTML tags for red color\n",
        "output_html = output_markdown.replace('<ins>', '<span style=\"color:red\">').replace('</ins>', '</span>')\n",
        "\n",
        "# Display the output as HTML\n",
        "display(HTML(output_html))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "APyIHF6ZvriJ",
        "outputId": "5ab80bf3-4791-412e-8305-bcc15ba38388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"color:red;font-weight:700;text-decoration:line-through;\">Got </span><span style=\"color:red;font-weight:700;\">I got </span>this for my <span style=\"color:red;font-weight:700;text-decoration:line-through;\">daughter for her </span><span style=\"color:red;font-weight:700;\">daughter's </span>birthday <span style=\"color:red;font-weight:700;text-decoration:line-through;\">cuz </span><span style=\"color:red;font-weight:700;\">because </span>she keeps taking mine from my <span style=\"color:red;font-weight:700;text-decoration:line-through;\">room.  </span><span style=\"color:red;font-weight:700;\">room. </span>Yes, adults also like pandas <span style=\"color:red;font-weight:700;text-decoration:line-through;\">too.  </span><span style=\"color:red;font-weight:700;\">too. </span>She takes it everywhere with her, and it's super soft and <span style=\"color:red;font-weight:700;text-decoration:line-through;\">cute.  One </span><span style=\"color:red;font-weight:700;\">cute. However, one </span>of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical. <span style=\"color:red;font-weight:700;text-decoration:line-through;\">It's </span><span style=\"color:red;font-weight:700;\">Additionally, it's </span>a bit small for what I paid for <span style=\"color:red;font-weight:700;text-decoration:line-through;\">it though. </span><span style=\"color:red;font-weight:700;\">it. </span>I think there might be other options that are bigger for the same <span style=\"color:red;font-weight:700;text-decoration:line-through;\">price.  It </span><span style=\"color:red;font-weight:700;\">price. On the positive side, it </span>arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "proofread and correct this review. Make it more compelling. \n",
        "Ensure it follows APA style guide and targets an advanced reader. \n",
        "Output in markdown format.\n",
        "Text: ```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "Nu2CdSLpxet6",
        "outputId": "7334642a-2015-46d6-c973-6c6266518ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Title: A Soft and Cute Panda Plush Toy for All Ages\n\nIntroduction:\nAs a parent, finding the perfect gift for your child's birthday can be a daunting task. However, I stumbled upon a soft and cute panda plush toy that not only made my daughter happy but also brought joy to me as an adult. In this review, I will share my experience with this product and provide an honest assessment of its features.\n\nProduct Description:\nThe panda plush toy is made of high-quality materials that make it super soft and cuddly. Its cute design is perfect for children and adults alike, making it a versatile gift option. The toy is small enough to carry around, making it an ideal companion for your child on their adventures.\n\nPros:\nThe panda plush toy is incredibly soft and cute, making it an excellent gift for children and adults. Its small size makes it easy to carry around, and its design is perfect for snuggling. The toy arrived a day earlier than expected, which was a pleasant surprise.\n\nCons:\nOne of the ears is a bit lower than the other, which makes the toy asymmetrical. Additionally, the toy is a bit small for its price, and there might be other options that are bigger for the same price.\n\nConclusion:\nOverall, the panda plush toy is an excellent gift option for children and adults who love cute and cuddly toys. Despite its small size and asymmetrical design, the toy's softness and cuteness make up for its shortcomings. I highly recommend this product to anyone looking for a versatile and adorable gift option."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expanding "
      ],
      "metadata": {
        "id": "mZ5f9-QCxrZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customize the automated reply to a customer email"
      ],
      "metadata": {
        "id": "TtANHoq-x8X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# given the sentiment from the lesson on \"inferring\",\n",
        "# and the original customer message, customize the email\n",
        "sentiment = \"negative\"\n",
        "\n",
        "# review for a blender\n",
        "review = f\"\"\"A waste of time with There is no trial feature. I came across\\\n",
        " this online and accessed their site for the first time.\\\n",
        "There was an option that said \"try dall-e,\" and I believed it was free to test,\\\n",
        "as are most apps and websites. When I registered, I verified my mobile, but it\\\n",
        "told me I didn't have any credits or credits. It was a complete waste of time and energy.\"\"\"\n"
      ],
      "metadata": {
        "id": "pGvuFExmxvxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are a customer service AI assistant.\n",
        "Your task is to send an email reply to a valued customer.\n",
        "Given the customer email delimited by ```, \\\n",
        "Generate a reply to thank the customer for their review.\n",
        "If the sentiment is positive or neutral, thank them for \\\n",
        "their review.\n",
        "If the sentiment is negative, apologize and suggest that \\\n",
        "they can reach out to customer service. \n",
        "Make sure to use specific details from the review.\n",
        "Write in a concise and professional tone.\n",
        "Sign the email as `AI customer agent`.\n",
        "Customer review: ```{review}```\n",
        "\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ2RLXuvyVRG",
        "outputId": "ae2c9e4c-6ee4-41a5-f28a-9637c3128266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear Valued Customer,\n",
            "\n",
            "Thank you for taking the time to share your experience with us. We are sorry to hear that you had a negative experience with our service. We apologize for any inconvenience caused by the lack of a trial feature. We understand how frustrating it can be to not have access to a feature that you were expecting.\n",
            "\n",
            "We appreciate your feedback and will take it into consideration as we continue to improve our service. If you have any further concerns or questions, please do not hesitate to reach out to our customer service team. They will be more than happy to assist you.\n",
            "\n",
            "Thank you again for your review and for choosing our service.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "AI customer agent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# given the sentiment from the lesson on \"inferring\",\n",
        "# and the original customer message, customize the email\n",
        "sentiment = \"positive\"\n",
        "\n",
        "# review for a blender\n",
        "review ='''\n",
        "ChatGPT, it's a great tool\\\n",
        " for any kind of business, even for developers. \\\n",
        " The truth, it can be used as a companion for your daily basis work,\\\n",
        "  but is not meant to be your lifeguard. You will need your own skills.\\\n",
        "   Don't expect to get a copywriter job 100% based on ChatGPT'''"
      ],
      "metadata": {
        "id": "8Fk9W9f5zH0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "your task is to reply an review as a customer service AI assisstant.\n",
        "given the customer email delimited by '''\n",
        "generate a reply to thank the customer for the review. \n",
        "If the sentiment is negative, say sorry and tell them to reach out the service\n",
        "if the sentiment is postive or neutral tahnk them for the review. \n",
        "make sure to use specific details in the review. Write in concise and proffesional way. \n",
        "sign the emial as 'ai customer agent'.\n",
        "review: '''{review}'''\n",
        "\"\"\"\n",
        "response=get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eHDFWsgzVQy",
        "outputId": "7d630117-b2ef-43b3-9c41-2b91ee851d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear valued customer,\n",
            "\n",
            "Thank you for taking the time to share your thoughts about ChatGPT. We are glad to hear that you find our tool useful for businesses and developers. We appreciate your feedback and are constantly working to improve our services.\n",
            "\n",
            "We understand that ChatGPT may not be the solution for every task, and we appreciate your honesty about its limitations. We strive to provide a tool that can assist with daily tasks, but we also encourage users to rely on their own skills and expertise.\n",
            "\n",
            "Thank you again for your review and for choosing ChatGPT. If you have any further concerns or questions, please do not hesitate to reach out to our customer service team for assistance.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "AI Customer Agent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reminding the model to use details from the customer's email\n"
      ],
      "metadata": {
        "id": "bzBojNk10LAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temperature \n",
        "\n",
        "It controls the randomness of the models output.\n",
        "If the temperature=0, the model will select the option which has highest percent.\n",
        "Highest perecentage = less chances option\n",
        "Ex: i like pizza then momo then samosa\n",
        "\n",
        "if temp=0 model will select pizza\n",
        "temp=0.3 model will select momo\n",
        "temp=0.7 model will select samosa \n",
        "\n",
        "to get more reliable, predictable response use temp=0\n",
        "to get output that has more variety then use high temp 0.3,0.7\n"
      ],
      "metadata": {
        "id": "KXh9RAr90iaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "You are a customer service AI assistant.\n",
        "Your task is to send an email reply to a valued customer.\n",
        "Given the customer email delimited by ```, \\\n",
        "Generate a reply to thank the customer for their review.\n",
        "If the sentiment is positive or neutral, thank them for \\\n",
        "their review.\n",
        "If the sentiment is negative, apologize and suggest that \\\n",
        "they can reach out to customer service. \n",
        "Make sure to use specific details from the review.\n",
        "Write in a concise and professional tone.\n",
        "Sign the email as `AI customer agent`.\n",
        "Customer review: ```{review}```\n",
        "\n",
        "\"\"\"\n",
        "temperature=0.7\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-mCqybb1nnA",
        "outputId": "79c18a81-0176-49ba-f5ac-8249ad3771e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear Valued Customer,\n",
            "\n",
            "Thank you for taking the time to share your thoughts about ChatGPT. We are thrilled to hear that you find our tool useful for any kind of business, including for developers. We appreciate your feedback and are glad to know that ChatGPT has been a helpful companion for your daily work.\n",
            "\n",
            "We understand that ChatGPT is not meant to replace your own skills, and we appreciate your honesty in acknowledging that. We apologize if there was any confusion about the tool's capabilities. We strive to provide our customers with the best possible experience, and we are always looking for ways to improve.\n",
            "\n",
            "Thank you again for your review, and please do not hesitate to reach out to our customer service team if you have any further questions or concerns. We are here to help.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "AI customer agent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "01aDkiHQ3a5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "#     print(str(response.choices[0].message))\n",
        "    return response.choices[0].message[\"content\"]\n"
      ],
      "metadata": {
        "id": "xd_ZL8Zw3hOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we are giving list of messages which can be variety of different roles."
      ],
      "metadata": {
        "id": "_hCvaO5B5iOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Roles of different messages\n",
        "1. system= which gives the instructions\n",
        "2. user\n",
        "3. assistant\n",
        "\n",
        "our messages which we type in chatgpt are called user messages \n",
        "the response we get frmo the model are called assistant messages.\n",
        "\n",
        "so the system messages helps in set the behaviour of assistant and it acts as high level instruction for the conversation.(guiding the responses of assistant messages) user wont be aware of system message\n",
        "\n",
        "[ system = sets behaviour of assistant \n",
        "  assistant = chat model\n",
        "  user = us ]"
      ],
      "metadata": {
        "id": "JPJ1J89D55lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  [  \n",
        "{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},    \n",
        "{'role':'user', 'content':'tell me a joke'},   \n",
        "{'role':'assistant', 'content':'Why did the chicken cross the road'},   \n",
        "{'role':'user', 'content':'I don\\'t know'}  ]"
      ],
      "metadata": {
        "id": "ZWbjvm3c7Dq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF6McKCp9Gm3",
        "outputId": "9210b77f-d385-406f-d240-4c14bbf3cc77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To get to the other side! A classic joke that doth never get old.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  [  \n",
        "{'role':'system', 'content':'You are friendly chatbot.'},    \n",
        "{'role':'user', 'content':'Hi, my name is Isa'}  ]\n",
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOevEyFo9jX_",
        "outputId": "d274ecf1-a866-4e57-a83c-bfaa51547921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Isa! I'm happy to chat with you. How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  [  \n",
        "{'role':'system', 'content':'You are friendly chatbot.'},    \n",
        "{'role':'user', 'content':'Yes,  can you remind me, What is my name?'}  ]\n",
        "response = get_completion_from_messages(messages, temperature=0)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AjqJCRG9wu9",
        "outputId": "2b47cc3f-5239-4fc8-dd25-5a3ab19e6faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but as an AI language model, I don't have access to your personal information, including your name. Can you please tell me your name?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  [  \n",
        "{'role':'system', 'content':'You are friendly chatbot.'},\n",
        "{'role':'user', 'content':'Hi, my name is Isa'},\n",
        "{'role':'assistant', 'content': \"Hi Isa! It's nice to meet you. \\\n",
        "Is there anything I can help you with today?\"},\n",
        "{'role':'user', 'content':'Yes, you can remind me, What is my name?'}  ]\n",
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jzHKUa19-Er",
        "outputId": "1e286d4e-f0ec-4f6e-d80d-9b3c8ecae703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your name is Isa!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the chatbot "
      ],
      "metadata": {
        "id": "6r-qnAkL-LdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "so here we automating the collection of user prompts and assistant responses.\n",
        "It is going to take orders at restaurant.\n",
        "we are defing a helper function which is collection of user messsages and also collecting the prompts from user interface. it append it to a list called context and then it will call the model with that context."
      ],
      "metadata": {
        "id": "Hbs6Lsk8-bmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_messages(_):\n",
        "    prompt = inp.value_input\n",
        "    inp.value = ''\n",
        "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
        "    response = get_completion_from_messages(context) \n",
        "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
        "    panels.append(\n",
        "        pn.Row('User:', pn.pane.Markdown(prompt, width=600)))\n",
        "    panels.append(\n",
        "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600, style={'background-color': '#F6F6F6'})))\n",
        " \n",
        "    return pn.Column(*panels)\n"
      ],
      "metadata": {
        "id": "XvuXCKZZ-Q4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "adding to the context, messages= system, user, assistant, user , assistant. this way model has the inforamtion on what to do next."
      ],
      "metadata": {
        "id": "IXur-8X__fEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "collect_messages() function handles user input, generates a response using the get_completion_from_messages() function, updates the conversation context, and displays the user's and assistant's messages in separate panels.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8n3Ik8FG_XMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import panel as pn  # GUI\n",
        "pn.extension()\n",
        "\n",
        "panels = [] # collect display \n",
        "\n",
        "context = [ {'role':'system', 'content':\"\"\"\n",
        "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
        "You first greet the customer, then collects the order, \\\n",
        "and then asks if it's a pickup or delivery. \\\n",
        "You wait to collect the entire order, then summarize it and check for a final \\\n",
        "time if the customer wants to add anything else. \\\n",
        "If it's a delivery, you ask for an address. \\\n",
        "Finally you collect the payment.\\\n",
        "Make sure to clarify all options, extras and sizes to uniquely \\\n",
        "identify the item from the menu.\\\n",
        "You respond in a short, very conversational friendly style. \\\n",
        "The menu includes \\\n",
        "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
        "cheese pizza   10.95, 9.25, 6.50 \\\n",
        "eggplant pizza   11.95, 9.75, 6.75 \\\n",
        "fries 4.50, 3.50 \\\n",
        "greek salad 7.25 \\\n",
        "Toppings: \\\n",
        "extra cheese 2.00, \\\n",
        "mushrooms 1.50 \\\n",
        "sausage 3.00 \\\n",
        "canadian bacon 3.50 \\\n",
        "AI sauce 1.50 \\\n",
        "peppers 1.00 \\\n",
        "Drinks: \\\n",
        "coke 3.00, 2.00, 1.00 \\\n",
        "sprite 3.00, 2.00, 1.00 \\\n",
        "bottled water 5.00 \\\n",
        "\"\"\"} ]  # accumulate messages\n",
        "\n",
        "\n",
        "inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text here…')\n",
        "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
        "\n",
        "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
        "\n",
        "dashboard = pn.Column(\n",
        "    inp,\n",
        "    pn.Row(button_conversation),\n",
        "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
        ")\n",
        "\n",
        "dashboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "OT115m4G_rgx",
        "outputId": "22b59f07-b31e-4f50-fee3-50a82fa48d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  var force = true;\n",
              "\n",
              "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
              "    root._bokeh_onload_callbacks = [];\n",
              "    root._bokeh_is_loading = undefined;\n",
              "  }\n",
              "\n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
              "        if (callback != null)\n",
              "          callback();\n",
              "      });\n",
              "    } finally {\n",
              "      delete root._bokeh_onload_callbacks\n",
              "    }\n",
              "    console.debug(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(css_urls, js_urls, js_modules, callback) {\n",
              "    if (css_urls == null) css_urls = [];\n",
              "    if (js_urls == null) js_urls = [];\n",
              "    if (js_modules == null) js_modules = [];\n",
              "\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    }\n",
              "    if (js_urls.length === 0 && js_modules.length === 0) {\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "\n",
              "    function on_load() {\n",
              "      root._bokeh_is_loading--;\n",
              "      if (root._bokeh_is_loading === 0) {\n",
              "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
              "        run_callbacks()\n",
              "      }\n",
              "    }\n",
              "\n",
              "    function on_error() {\n",
              "      console.error(\"failed to load \" + url);\n",
              "    }\n",
              "\n",
              "    for (var i = 0; i < css_urls.length; i++) {\n",
              "      var url = css_urls[i];\n",
              "      const element = document.createElement(\"link\");\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.rel = \"stylesheet\";\n",
              "      element.type = \"text/css\";\n",
              "      element.href = url;\n",
              "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
              "      document.body.appendChild(element);\n",
              "    }\n",
              "\n",
              "    var skip = [];\n",
              "    if (window.requirejs) {\n",
              "      window.requirejs.config({'packages': {}, 'paths': {'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'gridstack': {'exports': 'GridStack'}}});\n",
              "      require([\"gridstack\"], function(GridStack) {\n",
              "\twindow.GridStack = GridStack\n",
              "\ton_load()\n",
              "      })\n",
              "      require([\"notyf\"], function() {\n",
              "\ton_load()\n",
              "      })\n",
              "      root._bokeh_is_loading = css_urls.length + 2;\n",
              "    } else {\n",
              "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n",
              "    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n",
              "      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/gridstack/gridstack@4.2.5/dist/gridstack-h5.js'];\n",
              "      for (var i = 0; i < urls.length; i++) {\n",
              "        skip.push(urls[i])\n",
              "      }\n",
              "    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n",
              "      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n",
              "      for (var i = 0; i < urls.length; i++) {\n",
              "        skip.push(urls[i])\n",
              "      }\n",
              "    }    for (var i = 0; i < js_urls.length; i++) {\n",
              "      var url = js_urls[i];\n",
              "      if (skip.indexOf(url) >= 0) {\n",
              "\tif (!window.requirejs) {\n",
              "\t  on_load();\n",
              "\t}\n",
              "\tcontinue;\n",
              "      }\n",
              "      var element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "    for (var i = 0; i < js_modules.length; i++) {\n",
              "      var url = js_modules[i];\n",
              "      if (skip.indexOf(url) >= 0) {\n",
              "\tif (!window.requirejs) {\n",
              "\t  on_load();\n",
              "\t}\n",
              "\tcontinue;\n",
              "      }\n",
              "      var element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      element.type = \"module\";\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "    if (!js_urls.length && !js_modules.length) {\n",
              "      on_load()\n",
              "    }\n",
              "  };\n",
              "\n",
              "  function inject_raw_css(css) {\n",
              "    const element = document.createElement(\"style\");\n",
              "    element.appendChild(document.createTextNode(css));\n",
              "    document.body.appendChild(element);\n",
              "  }\n",
              "\n",
              "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n",
              "  var js_modules = [];\n",
              "  var css_urls = [\"https://cdn.holoviz.org/panel/0.14.4/dist/css/widgets.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/card.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/loading.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/debugger.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/markdown.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/dataframe.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/alerts.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/json.css\"];\n",
              "  var inline_js = [    function(Bokeh) {\n",
              "      inject_raw_css(\"\\n    .bk.pn-loading.arc:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n      background-size: auto calc(min(50%, 400px));\\n    }\\n    \");\n",
              "    },    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "function(Bokeh) {} // ensure no trailing comma for IE\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
              "      for (var i = 0; i < inline_js.length; i++) {\n",
              "        inline_js[i].call(root, root.Bokeh);\n",
              "      }} else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    }\n",
              "  }\n",
              "\n",
              "  if (root._bokeh_is_loading === 0) {\n",
              "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
              "    run_inline_js();\n",
              "  } else {\n",
              "    load_libs(css_urls, js_urls, js_modules, function() {\n",
              "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "      run_inline_js();\n",
              "    });\n",
              "  }\n",
              "}(window));"
            ],
            "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'gridstack': {'exports': 'GridStack'}}});\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 2;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/gridstack/gridstack@4.2.5/dist/gridstack-h5.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n  var js_modules = [];\n  var css_urls = [\"https://cdn.holoviz.org/panel/0.14.4/dist/css/widgets.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/card.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/loading.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/debugger.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/markdown.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/dataframe.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/alerts.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/json.css\"];\n  var inline_js = [    function(Bokeh) {\n      inject_raw_css(\"\\n    .bk.pn-loading.arc:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n      background-size: auto calc(min(50%, 400px));\\n    }\\n    \");\n    },    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, js_modules, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
            "application/javascript": [
              "\n",
              "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
              "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
              "}\n",
              "\n",
              "\n",
              "    function JupyterCommManager() {\n",
              "    }\n",
              "\n",
              "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
              "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
              "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
              "        comm_manager.register_target(comm_id, function(comm) {\n",
              "          comm.on_msg(msg_handler);\n",
              "        });\n",
              "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
              "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
              "          comm.onMsg = msg_handler;\n",
              "        });\n",
              "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
              "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
              "          var messages = comm.messages[Symbol.asyncIterator]();\n",
              "          function processIteratorResult(result) {\n",
              "            var message = result.value;\n",
              "            console.log(message)\n",
              "            var content = {data: message.data, comm_id};\n",
              "            var buffers = []\n",
              "            for (var buffer of message.buffers || []) {\n",
              "              buffers.push(new DataView(buffer))\n",
              "            }\n",
              "            var metadata = message.metadata || {};\n",
              "            var msg = {content, buffers, metadata}\n",
              "            msg_handler(msg);\n",
              "            return messages.next().then(processIteratorResult);\n",
              "          }\n",
              "          return messages.next().then(processIteratorResult);\n",
              "        })\n",
              "      }\n",
              "    }\n",
              "\n",
              "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
              "      if (comm_id in window.PyViz.comms) {\n",
              "        return window.PyViz.comms[comm_id];\n",
              "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
              "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
              "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
              "        if (msg_handler) {\n",
              "          comm.on_msg(msg_handler);\n",
              "        }\n",
              "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
              "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
              "        comm.open();\n",
              "        if (msg_handler) {\n",
              "          comm.onMsg = msg_handler;\n",
              "        }\n",
              "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
              "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
              "        comm_promise.then((comm) => {\n",
              "          window.PyViz.comms[comm_id] = comm;\n",
              "          if (msg_handler) {\n",
              "            var messages = comm.messages[Symbol.asyncIterator]();\n",
              "            function processIteratorResult(result) {\n",
              "              var message = result.value;\n",
              "              var content = {data: message.data};\n",
              "              var metadata = message.metadata || {comm_id};\n",
              "              var msg = {content, metadata}\n",
              "              msg_handler(msg);\n",
              "              return messages.next().then(processIteratorResult);\n",
              "            }\n",
              "            return messages.next().then(processIteratorResult);\n",
              "          }\n",
              "        }) \n",
              "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
              "          return comm_promise.then((comm) => {\n",
              "            comm.send(data, metadata, buffers, disposeOnDone);\n",
              "          });\n",
              "        };\n",
              "        var comm = {\n",
              "          send: sendClosure\n",
              "        };\n",
              "      }\n",
              "      window.PyViz.comms[comm_id] = comm;\n",
              "      return comm;\n",
              "    }\n",
              "    window.PyViz.comm_manager = new JupyterCommManager();\n",
              "    \n",
              "\n",
              "\n",
              "var JS_MIME_TYPE = 'application/javascript';\n",
              "var HTML_MIME_TYPE = 'text/html';\n",
              "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
              "var CLASS_NAME = 'output';\n",
              "\n",
              "/**\n",
              " * Render data to the DOM node\n",
              " */\n",
              "function render(props, node) {\n",
              "  var div = document.createElement(\"div\");\n",
              "  var script = document.createElement(\"script\");\n",
              "  node.appendChild(div);\n",
              "  node.appendChild(script);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle when a new output is added\n",
              " */\n",
              "function handle_add_output(event, handle) {\n",
              "  var output_area = handle.output_area;\n",
              "  var output = handle.output;\n",
              "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
              "    return\n",
              "  }\n",
              "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "  if (id !== undefined) {\n",
              "    var nchildren = toinsert.length;\n",
              "    var html_node = toinsert[nchildren-1].children[0];\n",
              "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "    var scripts = [];\n",
              "    var nodelist = html_node.querySelectorAll(\"script\");\n",
              "    for (var i in nodelist) {\n",
              "      if (nodelist.hasOwnProperty(i)) {\n",
              "        scripts.push(nodelist[i])\n",
              "      }\n",
              "    }\n",
              "\n",
              "    scripts.forEach( function (oldScript) {\n",
              "      var newScript = document.createElement(\"script\");\n",
              "      var attrs = [];\n",
              "      var nodemap = oldScript.attributes;\n",
              "      for (var j in nodemap) {\n",
              "        if (nodemap.hasOwnProperty(j)) {\n",
              "          attrs.push(nodemap[j])\n",
              "        }\n",
              "      }\n",
              "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
              "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
              "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
              "    });\n",
              "    if (JS_MIME_TYPE in output.data) {\n",
              "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
              "    }\n",
              "    output_area._hv_plot_id = id;\n",
              "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
              "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
              "    } else {\n",
              "      window.PyViz.plot_index[id] = null;\n",
              "    }\n",
              "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "    var bk_div = document.createElement(\"div\");\n",
              "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "    var script_attrs = bk_div.children[0].attributes;\n",
              "    for (var i = 0; i < script_attrs.length; i++) {\n",
              "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "    }\n",
              "    // store reference to server id on output_area\n",
              "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "  }\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle when an output is cleared or removed\n",
              " */\n",
              "function handle_clear_output(event, handle) {\n",
              "  var id = handle.cell.output_area._hv_plot_id;\n",
              "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
              "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
              "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
              "  if (server_id !== null) {\n",
              "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
              "    return;\n",
              "  } else if (comm !== null) {\n",
              "    comm.send({event_type: 'delete', 'id': id});\n",
              "  }\n",
              "  delete PyViz.plot_index[id];\n",
              "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
              "    var doc = window.Bokeh.index[id].model.document\n",
              "    doc.clear();\n",
              "    const i = window.Bokeh.documents.indexOf(doc);\n",
              "    if (i > -1) {\n",
              "      window.Bokeh.documents.splice(i, 1);\n",
              "    }\n",
              "  }\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle kernel restart event\n",
              " */\n",
              "function handle_kernel_cleanup(event, handle) {\n",
              "  delete PyViz.comms[\"hv-extension-comm\"];\n",
              "  window.PyViz.plot_index = {}\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle update_display_data messages\n",
              " */\n",
              "function handle_update_output(event, handle) {\n",
              "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
              "  handle_add_output(event, handle)\n",
              "}\n",
              "\n",
              "function register_renderer(events, OutputArea) {\n",
              "  function append_mime(data, metadata, element) {\n",
              "    // create a DOM node to render to\n",
              "    var toinsert = this.create_output_subarea(\n",
              "    metadata,\n",
              "    CLASS_NAME,\n",
              "    EXEC_MIME_TYPE\n",
              "    );\n",
              "    this.keyboard_manager.register_events(toinsert);\n",
              "    // Render to node\n",
              "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "    render(props, toinsert[0]);\n",
              "    element.append(toinsert);\n",
              "    return toinsert\n",
              "  }\n",
              "\n",
              "  events.on('output_added.OutputArea', handle_add_output);\n",
              "  events.on('output_updated.OutputArea', handle_update_output);\n",
              "  events.on('clear_output.CodeCell', handle_clear_output);\n",
              "  events.on('delete.Cell', handle_clear_output);\n",
              "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
              "\n",
              "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "    safe: true,\n",
              "    index: 0\n",
              "  });\n",
              "}\n",
              "\n",
              "if (window.Jupyter !== undefined) {\n",
              "  try {\n",
              "    var events = require('base/js/events');\n",
              "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  } catch(err) {\n",
              "  }\n",
              "}\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.bk-root, .bk-root .bk:before, .bk-root .bk:after {\n",
              "  font-family: var(--jp-ui-font-size1);\n",
              "  font-size: var(--jp-ui-font-size1);\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  var force = true;\n",
              "\n",
              "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
              "    root._bokeh_onload_callbacks = [];\n",
              "    root._bokeh_is_loading = undefined;\n",
              "  }\n",
              "\n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
              "        if (callback != null)\n",
              "          callback();\n",
              "      });\n",
              "    } finally {\n",
              "      delete root._bokeh_onload_callbacks\n",
              "    }\n",
              "    console.debug(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(css_urls, js_urls, js_modules, callback) {\n",
              "    if (css_urls == null) css_urls = [];\n",
              "    if (js_urls == null) js_urls = [];\n",
              "    if (js_modules == null) js_modules = [];\n",
              "\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    }\n",
              "    if (js_urls.length === 0 && js_modules.length === 0) {\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "\n",
              "    function on_load() {\n",
              "      root._bokeh_is_loading--;\n",
              "      if (root._bokeh_is_loading === 0) {\n",
              "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
              "        run_callbacks()\n",
              "      }\n",
              "    }\n",
              "\n",
              "    function on_error() {\n",
              "      console.error(\"failed to load \" + url);\n",
              "    }\n",
              "\n",
              "    for (var i = 0; i < css_urls.length; i++) {\n",
              "      var url = css_urls[i];\n",
              "      const element = document.createElement(\"link\");\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.rel = \"stylesheet\";\n",
              "      element.type = \"text/css\";\n",
              "      element.href = url;\n",
              "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
              "      document.body.appendChild(element);\n",
              "    }\n",
              "\n",
              "    var skip = [];\n",
              "    if (window.requirejs) {\n",
              "      window.requirejs.config({'packages': {}, 'paths': {'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'gridstack': {'exports': 'GridStack'}}});\n",
              "      require([\"gridstack\"], function(GridStack) {\n",
              "\twindow.GridStack = GridStack\n",
              "\ton_load()\n",
              "      })\n",
              "      require([\"notyf\"], function() {\n",
              "\ton_load()\n",
              "      })\n",
              "      root._bokeh_is_loading = css_urls.length + 2;\n",
              "    } else {\n",
              "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n",
              "    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n",
              "      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/gridstack/gridstack@4.2.5/dist/gridstack-h5.js'];\n",
              "      for (var i = 0; i < urls.length; i++) {\n",
              "        skip.push(urls[i])\n",
              "      }\n",
              "    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n",
              "      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n",
              "      for (var i = 0; i < urls.length; i++) {\n",
              "        skip.push(urls[i])\n",
              "      }\n",
              "    }    for (var i = 0; i < js_urls.length; i++) {\n",
              "      var url = js_urls[i];\n",
              "      if (skip.indexOf(url) >= 0) {\n",
              "\tif (!window.requirejs) {\n",
              "\t  on_load();\n",
              "\t}\n",
              "\tcontinue;\n",
              "      }\n",
              "      var element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "    for (var i = 0; i < js_modules.length; i++) {\n",
              "      var url = js_modules[i];\n",
              "      if (skip.indexOf(url) >= 0) {\n",
              "\tif (!window.requirejs) {\n",
              "\t  on_load();\n",
              "\t}\n",
              "\tcontinue;\n",
              "      }\n",
              "      var element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error;\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      element.type = \"module\";\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "    if (!js_urls.length && !js_modules.length) {\n",
              "      on_load()\n",
              "    }\n",
              "  };\n",
              "\n",
              "  function inject_raw_css(css) {\n",
              "    const element = document.createElement(\"style\");\n",
              "    element.appendChild(document.createTextNode(css));\n",
              "    document.body.appendChild(element);\n",
              "  }\n",
              "\n",
              "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n",
              "  var js_modules = [];\n",
              "  var css_urls = [\"https://cdn.holoviz.org/panel/0.14.4/dist/css/widgets.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/card.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/loading.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/debugger.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/markdown.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/dataframe.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/alerts.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/json.css\"];\n",
              "  var inline_js = [    function(Bokeh) {\n",
              "      inject_raw_css(\"\\n    .bk.pn-loading.arc:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n      background-size: auto calc(min(50%, 400px));\\n    }\\n    \");\n",
              "    },    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "function(Bokeh) {} // ensure no trailing comma for IE\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
              "      for (var i = 0; i < inline_js.length; i++) {\n",
              "        inline_js[i].call(root, root.Bokeh);\n",
              "      }} else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    }\n",
              "  }\n",
              "\n",
              "  if (root._bokeh_is_loading === 0) {\n",
              "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
              "    run_inline_js();\n",
              "  } else {\n",
              "    load_libs(css_urls, js_urls, js_modules, function() {\n",
              "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "      run_inline_js();\n",
              "    });\n",
              "  }\n",
              "}(window));"
            ],
            "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'gridstack': {'exports': 'GridStack'}}});\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 2;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/gridstack/gridstack@4.2.5/dist/gridstack-h5.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/0.14.4/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.14.4/dist/panel.min.js\"];\n  var js_modules = [];\n  var css_urls = [\"https://cdn.holoviz.org/panel/0.14.4/dist/css/widgets.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/card.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/loading.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/debugger.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/markdown.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/dataframe.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/alerts.css\", \"https://cdn.holoviz.org/panel/0.14.4/dist/css/json.css\"];\n  var inline_js = [    function(Bokeh) {\n      inject_raw_css(\"\\n    .bk.pn-loading.arc:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHN0eWxlPSJtYXJnaW46IGF1dG87IGJhY2tncm91bmQ6IG5vbmU7IGRpc3BsYXk6IGJsb2NrOyBzaGFwZS1yZW5kZXJpbmc6IGF1dG87IiB2aWV3Qm94PSIwIDAgMTAwIDEwMCIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPiAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgZmlsbD0ibm9uZSIgc3Ryb2tlPSIjYzNjM2MzIiBzdHJva2Utd2lkdGg9IjEwIiByPSIzNSIgc3Ryb2tlLWRhc2hhcnJheT0iMTY0LjkzMzYxNDMxMzQ2NDE1IDU2Ljk3Nzg3MTQzNzgyMTM4Ij4gICAgPGFuaW1hdGVUcmFuc2Zvcm0gYXR0cmlidXRlTmFtZT0idHJhbnNmb3JtIiB0eXBlPSJyb3RhdGUiIHJlcGVhdENvdW50PSJpbmRlZmluaXRlIiBkdXI9IjFzIiB2YWx1ZXM9IjAgNTAgNTA7MzYwIDUwIDUwIiBrZXlUaW1lcz0iMDsxIj48L2FuaW1hdGVUcmFuc2Zvcm0+ICA8L2NpcmNsZT48L3N2Zz4=\\\");\\n      background-size: auto calc(min(50%, 400px));\\n    }\\n    \");\n    },    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, js_modules, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
            "application/javascript": [
              "\n",
              "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
              "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
              "}\n",
              "\n",
              "\n",
              "    function JupyterCommManager() {\n",
              "    }\n",
              "\n",
              "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
              "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
              "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
              "        comm_manager.register_target(comm_id, function(comm) {\n",
              "          comm.on_msg(msg_handler);\n",
              "        });\n",
              "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
              "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
              "          comm.onMsg = msg_handler;\n",
              "        });\n",
              "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
              "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
              "          var messages = comm.messages[Symbol.asyncIterator]();\n",
              "          function processIteratorResult(result) {\n",
              "            var message = result.value;\n",
              "            console.log(message)\n",
              "            var content = {data: message.data, comm_id};\n",
              "            var buffers = []\n",
              "            for (var buffer of message.buffers || []) {\n",
              "              buffers.push(new DataView(buffer))\n",
              "            }\n",
              "            var metadata = message.metadata || {};\n",
              "            var msg = {content, buffers, metadata}\n",
              "            msg_handler(msg);\n",
              "            return messages.next().then(processIteratorResult);\n",
              "          }\n",
              "          return messages.next().then(processIteratorResult);\n",
              "        })\n",
              "      }\n",
              "    }\n",
              "\n",
              "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
              "      if (comm_id in window.PyViz.comms) {\n",
              "        return window.PyViz.comms[comm_id];\n",
              "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
              "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
              "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
              "        if (msg_handler) {\n",
              "          comm.on_msg(msg_handler);\n",
              "        }\n",
              "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
              "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
              "        comm.open();\n",
              "        if (msg_handler) {\n",
              "          comm.onMsg = msg_handler;\n",
              "        }\n",
              "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
              "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
              "        comm_promise.then((comm) => {\n",
              "          window.PyViz.comms[comm_id] = comm;\n",
              "          if (msg_handler) {\n",
              "            var messages = comm.messages[Symbol.asyncIterator]();\n",
              "            function processIteratorResult(result) {\n",
              "              var message = result.value;\n",
              "              var content = {data: message.data};\n",
              "              var metadata = message.metadata || {comm_id};\n",
              "              var msg = {content, metadata}\n",
              "              msg_handler(msg);\n",
              "              return messages.next().then(processIteratorResult);\n",
              "            }\n",
              "            return messages.next().then(processIteratorResult);\n",
              "          }\n",
              "        }) \n",
              "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
              "          return comm_promise.then((comm) => {\n",
              "            comm.send(data, metadata, buffers, disposeOnDone);\n",
              "          });\n",
              "        };\n",
              "        var comm = {\n",
              "          send: sendClosure\n",
              "        };\n",
              "      }\n",
              "      window.PyViz.comms[comm_id] = comm;\n",
              "      return comm;\n",
              "    }\n",
              "    window.PyViz.comm_manager = new JupyterCommManager();\n",
              "    \n",
              "\n",
              "\n",
              "var JS_MIME_TYPE = 'application/javascript';\n",
              "var HTML_MIME_TYPE = 'text/html';\n",
              "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
              "var CLASS_NAME = 'output';\n",
              "\n",
              "/**\n",
              " * Render data to the DOM node\n",
              " */\n",
              "function render(props, node) {\n",
              "  var div = document.createElement(\"div\");\n",
              "  var script = document.createElement(\"script\");\n",
              "  node.appendChild(div);\n",
              "  node.appendChild(script);\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle when a new output is added\n",
              " */\n",
              "function handle_add_output(event, handle) {\n",
              "  var output_area = handle.output_area;\n",
              "  var output = handle.output;\n",
              "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
              "    return\n",
              "  }\n",
              "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "  if (id !== undefined) {\n",
              "    var nchildren = toinsert.length;\n",
              "    var html_node = toinsert[nchildren-1].children[0];\n",
              "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "    var scripts = [];\n",
              "    var nodelist = html_node.querySelectorAll(\"script\");\n",
              "    for (var i in nodelist) {\n",
              "      if (nodelist.hasOwnProperty(i)) {\n",
              "        scripts.push(nodelist[i])\n",
              "      }\n",
              "    }\n",
              "\n",
              "    scripts.forEach( function (oldScript) {\n",
              "      var newScript = document.createElement(\"script\");\n",
              "      var attrs = [];\n",
              "      var nodemap = oldScript.attributes;\n",
              "      for (var j in nodemap) {\n",
              "        if (nodemap.hasOwnProperty(j)) {\n",
              "          attrs.push(nodemap[j])\n",
              "        }\n",
              "      }\n",
              "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
              "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
              "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
              "    });\n",
              "    if (JS_MIME_TYPE in output.data) {\n",
              "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
              "    }\n",
              "    output_area._hv_plot_id = id;\n",
              "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
              "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
              "    } else {\n",
              "      window.PyViz.plot_index[id] = null;\n",
              "    }\n",
              "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "    var bk_div = document.createElement(\"div\");\n",
              "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "    var script_attrs = bk_div.children[0].attributes;\n",
              "    for (var i = 0; i < script_attrs.length; i++) {\n",
              "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "    }\n",
              "    // store reference to server id on output_area\n",
              "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "  }\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle when an output is cleared or removed\n",
              " */\n",
              "function handle_clear_output(event, handle) {\n",
              "  var id = handle.cell.output_area._hv_plot_id;\n",
              "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
              "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
              "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
              "  if (server_id !== null) {\n",
              "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
              "    return;\n",
              "  } else if (comm !== null) {\n",
              "    comm.send({event_type: 'delete', 'id': id});\n",
              "  }\n",
              "  delete PyViz.plot_index[id];\n",
              "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
              "    var doc = window.Bokeh.index[id].model.document\n",
              "    doc.clear();\n",
              "    const i = window.Bokeh.documents.indexOf(doc);\n",
              "    if (i > -1) {\n",
              "      window.Bokeh.documents.splice(i, 1);\n",
              "    }\n",
              "  }\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle kernel restart event\n",
              " */\n",
              "function handle_kernel_cleanup(event, handle) {\n",
              "  delete PyViz.comms[\"hv-extension-comm\"];\n",
              "  window.PyViz.plot_index = {}\n",
              "}\n",
              "\n",
              "/**\n",
              " * Handle update_display_data messages\n",
              " */\n",
              "function handle_update_output(event, handle) {\n",
              "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
              "  handle_add_output(event, handle)\n",
              "}\n",
              "\n",
              "function register_renderer(events, OutputArea) {\n",
              "  function append_mime(data, metadata, element) {\n",
              "    // create a DOM node to render to\n",
              "    var toinsert = this.create_output_subarea(\n",
              "    metadata,\n",
              "    CLASS_NAME,\n",
              "    EXEC_MIME_TYPE\n",
              "    );\n",
              "    this.keyboard_manager.register_events(toinsert);\n",
              "    // Render to node\n",
              "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "    render(props, toinsert[0]);\n",
              "    element.append(toinsert);\n",
              "    return toinsert\n",
              "  }\n",
              "\n",
              "  events.on('output_added.OutputArea', handle_add_output);\n",
              "  events.on('output_updated.OutputArea', handle_update_output);\n",
              "  events.on('clear_output.CodeCell', handle_clear_output);\n",
              "  events.on('delete.Cell', handle_clear_output);\n",
              "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
              "\n",
              "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "    safe: true,\n",
              "    index: 0\n",
              "  });\n",
              "}\n",
              "\n",
              "if (window.Jupyter !== undefined) {\n",
              "  try {\n",
              "    var events = require('base/js/events');\n",
              "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  } catch(err) {\n",
              "  }\n",
              "}\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.bk-root, .bk-root .bk:before, .bk-root .bk:after {\n",
              "  font-family: var(--jp-ui-font-size1);\n",
              "  font-size: var(--jp-ui-font-size1);\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {},
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div id='1002'>\n",
              "  <div class=\"bk-root\" id=\"953f4abd-7927-472e-b6ff-1e111b341786\" data-root-id=\"1002\"></div>\n",
              "</div>\n",
              "<script type=\"application/javascript\">(function(root) {\n",
              "  function embed_document(root) {\n",
              "    var docs_json = {\"d602bbf9-e2bc-412c-9ffa-b934f9d47cd1\":{\"defs\":[{\"extends\":null,\"module\":null,\"name\":\"ReactiveHTML1\",\"overrides\":[],\"properties\":[]},{\"extends\":null,\"module\":null,\"name\":\"FlexBox1\",\"overrides\":[],\"properties\":[{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_content\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_items\"},{\"default\":\"row\",\"kind\":null,\"name\":\"flex_direction\"},{\"default\":\"wrap\",\"kind\":null,\"name\":\"flex_wrap\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"justify_content\"}]},{\"extends\":null,\"module\":null,\"name\":\"GridStack1\",\"overrides\":[],\"properties\":[{\"default\":\"warn\",\"kind\":null,\"name\":\"mode\"},{\"default\":null,\"kind\":null,\"name\":\"ncols\"},{\"default\":null,\"kind\":null,\"name\":\"nrows\"},{\"default\":true,\"kind\":null,\"name\":\"allow_resize\"},{\"default\":true,\"kind\":null,\"name\":\"allow_drag\"},{\"default\":[],\"kind\":null,\"name\":\"state\"}]},{\"extends\":null,\"module\":null,\"name\":\"click1\",\"overrides\":[],\"properties\":[{\"default\":\"\",\"kind\":null,\"name\":\"terminal_output\"},{\"default\":\"\",\"kind\":null,\"name\":\"debug_name\"},{\"default\":0,\"kind\":null,\"name\":\"clears\"}]},{\"extends\":null,\"module\":null,\"name\":\"NotificationAreaBase1\",\"overrides\":[],\"properties\":[{\"default\":\"bottom-right\",\"kind\":null,\"name\":\"position\"},{\"default\":0,\"kind\":null,\"name\":\"_clear\"}]},{\"extends\":null,\"module\":null,\"name\":\"NotificationArea1\",\"overrides\":[],\"properties\":[{\"default\":[],\"kind\":null,\"name\":\"notifications\"},{\"default\":\"bottom-right\",\"kind\":null,\"name\":\"position\"},{\"default\":0,\"kind\":null,\"name\":\"_clear\"},{\"default\":[{\"background\":\"#ffc107\",\"icon\":{\"className\":\"fas fa-exclamation-triangle\",\"color\":\"white\",\"tagName\":\"i\"},\"type\":\"warning\"},{\"background\":\"#007bff\",\"icon\":{\"className\":\"fas fa-info-circle\",\"color\":\"white\",\"tagName\":\"i\"},\"type\":\"info\"}],\"kind\":null,\"name\":\"types\"}]},{\"extends\":null,\"module\":null,\"name\":\"Notification\",\"overrides\":[],\"properties\":[{\"default\":null,\"kind\":null,\"name\":\"background\"},{\"default\":3000,\"kind\":null,\"name\":\"duration\"},{\"default\":null,\"kind\":null,\"name\":\"icon\"},{\"default\":\"\",\"kind\":null,\"name\":\"message\"},{\"default\":null,\"kind\":null,\"name\":\"notification_type\"},{\"default\":false,\"kind\":null,\"name\":\"_destroyed\"}]},{\"extends\":null,\"module\":null,\"name\":\"TemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]},{\"extends\":null,\"module\":null,\"name\":\"MaterialTemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]}],\"roots\":{\"references\":[{\"attributes\":{\"css_classes\":[\"markdown\"],\"margin\":[5,5,5,5],\"name\":\"Markdown00112\",\"text\":\"&lt;p&gt;User:&lt;/p&gt;\"},\"id\":\"1009\",\"type\":\"panel.models.markup.HTML\"},{\"attributes\":{\"icon\":null,\"js_event_callbacks\":{\"button_click\":[{\"id\":\"1014\"}]},\"label\":\"Chat!\",\"margin\":[5,10,5,10],\"subscribed_events\":[\"button_click\"]},\"id\":\"1005\",\"type\":\"Button\"},{\"attributes\":{\"children\":[{\"id\":\"1009\"},{\"id\":\"1010\"}],\"margin\":[0,0,0,0],\"name\":\"Row00114\"},\"id\":\"1008\",\"type\":\"Row\"},{\"attributes\":{\"children\":[{\"id\":\"1008\"},{\"id\":\"1011\"}],\"margin\":[0,0,0,0],\"name\":\"Column00120\"},\"id\":\"1007\",\"type\":\"Column\"},{\"attributes\":{\"children\":[{\"id\":\"1012\"},{\"id\":\"1013\"}],\"margin\":[0,0,0,0],\"name\":\"Row00119\"},\"id\":\"1011\",\"type\":\"Row\"},{\"attributes\":{\"css_classes\":[\"markdown\"],\"margin\":[5,5,5,5],\"name\":\"Markdown00110\",\"width\":600},\"id\":\"1010\",\"type\":\"panel.models.markup.HTML\"},{\"attributes\":{\"css_classes\":[\"markdown\"],\"margin\":[5,5,5,5],\"name\":\"Markdown00117\",\"text\":\"&lt;p&gt;Assistant:&lt;/p&gt;\"},\"id\":\"1012\",\"type\":\"panel.models.markup.HTML\"},{\"attributes\":{\"css_classes\":[\"markdown\"],\"margin\":[5,5,5,5],\"name\":\"Markdown00115\",\"style\":{\"background-color\":\"#F6F6F6\"},\"text\":\"&lt;p&gt;Hello! Welcome to our pizza restaurant. What can I get for you today?&lt;/p&gt;\",\"width\":600},\"id\":\"1013\",\"type\":\"panel.models.markup.HTML\"},{\"attributes\":{\"reload\":false},\"id\":\"1016\",\"type\":\"panel.models.location.Location\"},{\"attributes\":{\"margin\":[5,10,5,10],\"max_length\":5000,\"placeholder\":\"Enter text here\\u2026\"},\"id\":\"1003\",\"type\":\"TextInput\"},{\"attributes\":{\"client_comm_id\":\"d4f87ec1fbd64e82823c28eadfd7a56f\",\"comm_id\":\"cd1abcf2afd548349385e16f367b1ab4\",\"plot_id\":\"1002\"},\"id\":\"1015\",\"type\":\"panel.models.comm_manager.CommManager\"},{\"attributes\":{\"children\":[{\"id\":\"1007\"}],\"height\":300,\"margin\":[0,0,0,0],\"min_height\":300,\"name\":\"Row00108\"},\"id\":\"1006\",\"type\":\"Row\"},{\"attributes\":{\"children\":[{\"id\":\"1005\"}],\"margin\":[0,0,0,0],\"name\":\"Row00103\"},\"id\":\"1004\",\"type\":\"Row\"},{\"attributes\":{\"children\":[{\"id\":\"1003\"},{\"id\":\"1004\"},{\"id\":\"1006\"}],\"margin\":[0,0,0,0],\"name\":\"Column00122\"},\"id\":\"1002\",\"type\":\"Column\"},{\"attributes\":{\"args\":{\"bidirectional\":false,\"properties\":{\"event:button_click\":\"loading\"},\"source\":{\"id\":\"1005\"},\"target\":{\"id\":\"1006\"}},\"code\":\"\\n    if ('event:button_click'.startsWith('event:')) {\\n      var value = true\\n    } else {\\n      var value = source['event:button_click'];\\n      value = value;\\n    }\\n    if (typeof value !== 'boolean' || source.labels !== ['Loading']) {\\n      value = true\\n    }\\n    var css_classes = target.css_classes.slice()\\n    var loading_css = ['pn-loading', 'arc']\\n    if (value) {\\n      for (var css of loading_css) {\\n        if (!(css in css_classes)) {\\n          css_classes.push(css)\\n        }\\n      }\\n    } else {\\n     for (var css of loading_css) {\\n        var index = css_classes.indexOf(css)\\n        if (index > -1) {\\n          css_classes.splice(index, 1)\\n        }\\n      }\\n    }\\n    target['css_classes'] = css_classes\\n    \",\"tags\":[[140277850898000,[null,\"event:button_click\"],[null,\"loading\"]]]},\"id\":\"1014\",\"type\":\"CustomJS\"}],\"root_ids\":[\"1002\",\"1015\",\"1016\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.3\"}};\n",
              "    var render_items = [{\"docid\":\"d602bbf9-e2bc-412c-9ffa-b934f9d47cd1\",\"root_ids\":[\"1002\"],\"roots\":{\"1002\":\"953f4abd-7927-472e-b6ff-1e111b341786\"}}];\n",
              "    root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "    for (const render_item of render_items) {\n",
              "      for (const root_id of render_item.root_ids) {\n",
              "\tconst id_el = document.getElementById(root_id)\n",
              "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
              "\t  const root_el = id_el.children[0]\n",
              "\t  root_el.id = root_el.id + '-rendered'\n",
              "\t}\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "  if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    var attempts = 0;\n",
              "    var timer = setInterval(function(root) {\n",
              "      if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
              "        clearInterval(timer);\n",
              "        embed_document(root);\n",
              "      } else if (document.readyState == \"complete\") {\n",
              "        attempts++;\n",
              "        if (attempts > 200) {\n",
              "          clearInterval(timer);\n",
              "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
              "        }\n",
              "      }\n",
              "    }, 25, root)\n",
              "  }\n",
              "})(window);</script>"
            ],
            "application/vnd.holoviews_exec.v0+json": "",
            "text/plain": [
              "Column\n",
              "    [0] TextInput(placeholder='Enter text here…')\n",
              "    [1] Row\n",
              "        [0] Button(name='Chat!')\n",
              "    [2] ParamFunction(function, _pane=Column, height=300, loading_indicator=True)"
            ]
          },
          "metadata": {
            "application/vnd.holoviews_exec.v0+json": {
              "id": "1002"
            }
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "context contains the list of system messages which provides instruction to the chatbot.\n",
        "\n",
        "inp = this is text input widget where user cna enter text.\n",
        "\n",
        "we need button like to enter so we name it as chat.\n",
        "to create an interactive conversation panel we use interative_conversation and applying collect mesages function \n",
        "\n",
        "to create dashboard we use pn.column function, which includes input, chat button, and interactive conversation panel. \n",
        "and then we display the dashboard.\n",
        "\n"
      ],
      "metadata": {
        "id": "modP6l3MAOd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "code sets up a GUI with a text input field, a chat button, and a panel to display the conversation with the AI bot. The collect_messages() function handles the logic of processing user input and generating bot responses.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wZIIUhqVBmq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "system messages = we are telling that it is orderbot to collect pizza. we give it what menu includes "
      ],
      "metadata": {
        "id": "o_S5UpIrB-fI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  context.copy()\n",
        "messages.append(\n",
        "{'role':'system', 'content':'create a json summary of the previous food order. Itemize the price for each item\\\n",
        " The fields should be 1) pizza, include size 2) list of toppings 3) list of drinks, include size   4) list of sides include size  5)total price '},    \n",
        ")\n",
        " #The fields should be 1) pizza, price 2) list of toppings 3) list of drinks, include size include price  4) list of sides include size include price, 5)total price '},    \n",
        "\n",
        "response = get_completion_from_messages(messages, temperature=0)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qNC_1r-Cujm",
        "outputId": "afa64b2a-5557-4620-f618-692da5b0f168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's a JSON summary of your order:\n",
            "\n",
            "```\n",
            "{\n",
            "  \"pizza\": {\n",
            "    \"type\": \"cheese\",\n",
            "    \"size\": \"small\",\n",
            "    \"toppings\": [\n",
            "      \"extra cheese\",\n",
            "      \"mushrooms\",\n",
            "      \"sausage\",\n",
            "      \"Canadian bacon\",\n",
            "      \"AI sauce\",\n",
            "      \"peppers\"\n",
            "    ],\n",
            "    \"price\": 24.95\n",
            "  },\n",
            "  \"drinks\": [],\n",
            "  \"sides\": [],\n",
            "  \"total_price\": 24.95\n",
            "}\n",
            "```\n",
            "\n",
            "Please note that the total price does not include any drinks or sides. If you would like to add any drinks or sides, please let me know.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here messages varibales is a copy of context list which has conversation between user and chatbot above. \n",
        "it then appends new message list instructing to create json summary of previous order and prize.\n"
      ],
      "metadata": {
        "id": "DSr2bxq6DSSs"
      }
    }
  ]
}